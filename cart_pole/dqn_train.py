#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DQN Training Script for CartPole
åŸºäºDQNçš„CartPoleè®­ç»ƒè„šæœ¬
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
from collections import deque
import matplotlib.pyplot as plt
from cart_pole_env import CartPoleWrapper


class DQN(nn.Module):
    """
    Deep Q-Network (DQN) ç¥ç»ç½‘ç»œ
    """
    
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        """
        åˆå§‹åŒ–DQNç½‘ç»œ
        
        Args:
            state_dim: çŠ¶æ€ç©ºé—´ç»´åº¦ (CartPoleä¸º4)
            action_dim: åŠ¨ä½œç©ºé—´ç»´åº¦ (CartPoleä¸º2)
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


class ReplayBuffer:
    """
    ç»éªŒå›æ”¾ç¼“å†²åŒº
    """
    
    def __init__(self, capacity=10000):
        """
        åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒº
        
        Args:
            capacity: ç¼“å†²åŒºå®¹é‡
        """
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """
        æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº
        
        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
        """
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        """
        ä»ç¼“å†²åŒºéšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            states, actions, rewards, next_states, dones
        """
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.BoolTensor(dones)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        """è¿”å›ç¼“å†²åŒºå½“å‰å¤§å°"""
        return len(self.buffer)


class DQNAgent:
    """
    DQNæ™ºèƒ½ä½“
    """
    
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, 
                 epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995,
                 memory_size=10000, batch_size=64, target_update=100):
        """
        åˆå§‹åŒ–DQNæ™ºèƒ½ä½“
        
        Args:
            state_dim: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_dim: åŠ¨ä½œç©ºé—´ç»´åº¦
            lr: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            epsilon: åˆå§‹æ¢ç´¢ç‡
            epsilon_min: æœ€å°æ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢ç‡è¡°å‡
            memory_size: ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
            batch_size: æ‰¹æ¬¡å¤§å°
            target_update: ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update = target_update
        self.update_counter = 0
        
        # åˆ›å»ºä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_network = DQN(state_dim, action_dim)
        self.target_network = DQN(state_dim, action_dim)
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.target_network.eval()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = ReplayBuffer(memory_size)
        
        # è®­ç»ƒå†å²
        self.episode_rewards = []
        self.episode_lengths = []
    
    def select_action(self, state, training=True):
        """
        ä½¿ç”¨epsilon-greedyç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ï¼ˆå½±å“æ¢ç´¢ï¼‰
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
        """
        if training and random.random() < self.epsilon:
            # éšæœºæ¢ç´¢
            return random.randrange(self.action_dim)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        self.memory.push(state, action, reward, next_state, done)
    
    def train_step(self):
        """
        æ‰§è¡Œä¸€æ­¥è®­ç»ƒ - DQNç®—æ³•çš„æ ¸å¿ƒè®­ç»ƒæ­¥éª¤
        
        å®ç°æµç¨‹ï¼š
        1. ä»ç»éªŒå›æ”¾ç¼“å†²åŒºé‡‡æ ·ä¸€æ‰¹ç»éªŒ
        2. ä½¿ç”¨ä¸»ç½‘ç»œè®¡ç®—å½“å‰Qå€¼ Q(s,a)
        3. ä½¿ç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—ç›®æ ‡Qå€¼ r + Î³ * max Q(s',a')
        4. è®¡ç®—MSEæŸå¤±å¹¶åå‘ä¼ æ’­æ›´æ–°ä¸»ç½‘ç»œ
        5. å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°
        6. è¡°å‡æ¢ç´¢ç‡epsilon
        
        Returns:
            loss: æŸå¤±å€¼ï¼ˆå¦‚æœè®­ç»ƒæˆåŠŸï¼‰ï¼Œå¦åˆ™è¿”å›None
        """
        # æ­¥éª¤1: æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬
        # è®­ç»ƒåˆæœŸæ ·æœ¬ä¸è¶³æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡è®­ç»ƒ
        if len(self.memory) < self.batch_size:
            return None
        
        # æ­¥éª¤2: ä»ç»éªŒå›æ”¾ç¼“å†²åŒºéšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ
        # è¿”å›: states[batch, 4], actions[batch], rewards[batch], 
        #       next_states[batch, 4], dones[batch]
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        # æ­¥éª¤3: ä½¿ç”¨ä¸»ç½‘ç»œè®¡ç®—å½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹çš„Qå€¼
        # q_network(states) -> [batch, 2] (æ¯ä¸ªçŠ¶æ€å¯¹åº”2ä¸ªåŠ¨ä½œçš„Qå€¼)
        # gather(1, actions) -> [batch, 1] (åªæå–å®é™…æ‰§è¡ŒåŠ¨ä½œçš„Qå€¼)
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # æ­¥éª¤4: ä½¿ç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—ç›®æ ‡Qå€¼ï¼ˆBellmanæ–¹ç¨‹ï¼‰
        # ä½¿ç”¨torch.no_grad()ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œå› ä¸ºç›®æ ‡ç½‘ç»œä¸éœ€è¦æ›´æ–°
        with torch.no_grad():
            # è®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼: max_a' Q_target(s', a')
            # max(1)[0] åœ¨åŠ¨ä½œç»´åº¦ä¸Šå–æœ€å¤§å€¼ï¼Œè¿”å› [batch]
            next_q_values = self.target_network(next_states).max(1)[0]
            
            # Bellmanæ–¹ç¨‹: Q_target = r + Î³ * max Q(s',a') * (1 - done)
            # ~dones: å¦‚æœå›åˆç»“æŸï¼Œæœªæ¥å¥–åŠ±ä¸º0
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # æ­¥éª¤5: è®¡ç®—æŸå¤±ï¼ˆå½“å‰Qå€¼ä¸ç›®æ ‡Qå€¼çš„å‡æ–¹è¯¯å·®ï¼‰
        # squeeze()å°†[batch, 1]å‹ç¼©ä¸º[batch]ï¼Œä¸target_q_valueså½¢çŠ¶åŒ¹é…
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # æ­¥éª¤6: åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
        self.optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦ï¼ˆé˜²æ­¢æ¢¯åº¦ç´¯ç§¯ï¼‰
        loss.backward()              # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
        # æ¢¯åº¦è£å‰ªï¼šé™åˆ¶æ¢¯åº¦èŒƒæ•°æœ€å¤§ä¸º1.0ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()        # ä½¿ç”¨Adamä¼˜åŒ–å™¨æ›´æ–°å‚æ•°
        
        # æ­¥éª¤7: å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆæ¯target_updateæ­¥æ›´æ–°ä¸€æ¬¡ï¼‰
        # ç›®æ ‡ç½‘ç»œæ›´æ–°è¾ƒæ…¢ï¼Œæä¾›ç¨³å®šçš„ç›®æ ‡å€¼ï¼Œé¿å…è®­ç»ƒä¸ç¨³å®š
        self.update_counter += 1
        if self.update_counter % self.target_update == 0:
            # å°†ä¸»ç½‘ç»œçš„å‚æ•°å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œ
            self.target_network.load_state_dict(self.q_network.state_dict())
        
        # æ­¥éª¤8: è¡°å‡æ¢ç´¢ç‡epsilon
        # è®­ç»ƒåˆæœŸå¤šæ¢ç´¢ï¼ŒåæœŸå¤šåˆ©ç”¨å­¦åˆ°çš„ç­–ç•¥
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # è¿”å›æŸå¤±å€¼ï¼ˆç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼‰
        return loss.item()
    
    def save(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
        }, filepath)
        print(f"âœ… Model saved to {filepath}")
    
    def load(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.epsilon = checkpoint.get('epsilon', self.epsilon_min)
        self.episode_rewards = checkpoint.get('episode_rewards', [])
        self.episode_lengths = checkpoint.get('episode_lengths', [])
        print(f"âœ… Model loaded from {filepath}")


def train_dqn(env, agent, num_episodes=500, max_steps=500, save_interval=50):
    """
    è®­ç»ƒDQNæ™ºèƒ½ä½“
    
    Args:
        env: CartPoleç¯å¢ƒ
        agent: DQNæ™ºèƒ½ä½“
        num_episodes: è®­ç»ƒå›åˆæ•°
        max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
        save_interval: ä¿å­˜æ¨¡å‹çš„é—´éš”
    """
    print("ğŸš€ Starting DQN Training...")
    print(f"   Episodes: {num_episodes}")
    print(f"   Max steps per episode: {max_steps}")
    print(f"   Initial epsilon: {agent.epsilon:.3f}")
    print("-" * 60)
    
    for episode in range(num_episodes):
        state, info = env.reset()
        total_reward = 0
        steps = 0
        episode_losses = []
        
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # å­˜å‚¨ç»éªŒ
            agent.remember(state, action, reward, next_state, done)
            
            # è®­ç»ƒ
            loss = agent.train_step()
            if loss is not None:
                episode_losses.append(loss)
            
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        # è®°å½•å†å²
        agent.episode_rewards.append(total_reward)
        agent.episode_lengths.append(steps)
        
        # æ‰“å°è¿›åº¦
        avg_loss = np.mean(episode_losses) if episode_losses else 0
        avg_reward = np.mean(agent.episode_rewards[-10:]) if len(agent.episode_rewards) >= 10 else total_reward
        
        if (episode + 1) % 10 == 0:
            print(f"Episode {episode + 1:4d} | "
                  f"Reward: {total_reward:6.1f} | "
                  f"Steps: {steps:3d} | "
                  f"Epsilon: {agent.epsilon:.3f} | "
                  f"Avg Reward (10): {avg_reward:6.1f} | "
                  f"Loss: {avg_loss:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        if (episode + 1) % save_interval == 0:
            agent.save(f"dqn_model_episode_{episode + 1}.pth")
    
    print("-" * 60)
    print("âœ… Training completed!")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save("dqn_model_final.pth")
    
    return agent


def plot_training_history(agent, save_path="training_history.png"):
    """
    ç»˜åˆ¶è®­ç»ƒå†å²
    
    Args:
        agent: è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
        save_path: ä¿å­˜è·¯å¾„
    """
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # ç»˜åˆ¶å¥–åŠ±æ›²çº¿
    ax1.plot(agent.episode_rewards, alpha=0.6, label='Episode Reward')
    if len(agent.episode_rewards) >= 10:
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        window = 10
        moving_avg = np.convolve(agent.episode_rewards, 
                                 np.ones(window)/window, mode='valid')
        ax1.plot(range(window-1, len(agent.episode_rewards)), 
                moving_avg, 'r-', label=f'Moving Average ({window})', linewidth=2)
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Reward')
    ax1.set_title('Training Rewards')
    ax1.legend()
    ax1.grid(True)
    
    # ç»˜åˆ¶æ­¥æ•°æ›²çº¿
    ax2.plot(agent.episode_lengths, alpha=0.6, label='Episode Length')
    if len(agent.episode_lengths) >= 10:
        window = 10
        moving_avg = np.convolve(agent.episode_lengths, 
                                 np.ones(window)/window, mode='valid')
        ax2.plot(range(window-1, len(agent.episode_lengths)), 
                moving_avg, 'r-', label=f'Moving Average ({window})', linewidth=2)
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('Steps')
    ax2.set_title('Episode Lengths')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    print(f"ğŸ“Š Training history saved to {save_path}")
    plt.close()


def test_agent(env, agent, num_episodes=5, render=False):
    """
    æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    
    Args:
        env: CartPoleç¯å¢ƒ
        agent: è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
        num_episodes: æµ‹è¯•å›åˆæ•°
        render: æ˜¯å¦æ¸²æŸ“
    """
    print(f"\nğŸ§ª Testing agent for {num_episodes} episodes...")
    
    test_rewards = []
    test_lengths = []
    
    for episode in range(num_episodes):
        state, info = env.reset()
        total_reward = 0
        steps = 0
        done = False
        
        while not done:
            # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰
            action = agent.select_action(state, training=False)
            state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            total_reward += reward
            steps += 1
            
            if render:
                env.render()
        
        test_rewards.append(total_reward)
        test_lengths.append(steps)
        print(f"  Episode {episode + 1}: Reward = {total_reward:.1f}, Steps = {steps}")
    
    avg_reward = np.mean(test_rewards)
    avg_length = np.mean(test_lengths)
    print(f"\nğŸ“Š Test Results:")
    print(f"   Average Reward: {avg_reward:.2f}")
    print(f"   Average Steps: {avg_length:.2f}")
    print(f"   Max Reward: {max(test_rewards):.2f}")
    print(f"   Min Reward: {min(test_rewards):.2f}")


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºç¯å¢ƒ
    print("ğŸ® Creating CartPole environment...")
    env = CartPoleWrapper(render_mode=None)
    print(env)
    print()
    
    # è·å–ç¯å¢ƒä¿¡æ¯
    state_dim = env.get_state_dim()
    action_dim = env.get_action_dim()
    
    # åˆ›å»ºDQNæ™ºèƒ½ä½“
    print("ğŸ¤– Creating DQN agent...")
    agent = DQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        lr=0.001,
        gamma=0.99,
        epsilon=1.0,
        epsilon_min=0.01,
        epsilon_decay=0.995,
        memory_size=10000,
        batch_size=64,
        target_update=100
    )
    print("âœ… Agent created!")
    print()
    
    # è®­ç»ƒ
    agent = train_dqn(env, agent, num_episodes=500, max_steps=500, save_interval=50)
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(agent)
    
    # æµ‹è¯•
    test_agent(env, agent, num_episodes=10, render=False)
    
    # å…³é—­ç¯å¢ƒ
    env.close()
    print("\nğŸ‰ All done!")


if __name__ == "__main__":
    main()

