#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Evaluation Script for Behavioral Cloning
è¡Œä¸ºå…‹éš†è¯„ä¼°è„šæœ¬

è¿™ä¸ªè„šæœ¬è´Ÿè´£ï¼š
- åŠ è½½è®­ç»ƒå¥½çš„è¡Œä¸ºå…‹éš†æ¨¡å‹
- è¿›è¡Œå…¨é¢çš„æ€§èƒ½è¯„ä¼°
- ä¸ä¸“å®¶ç­–ç•¥è¿›è¡Œå¯¹æ¯”
- ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šå’Œå¯è§†åŒ–
"""

import argparse
import json
import os
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import torch
from datetime import datetime

from data_collection import ImitationDataset
from behavioral_cloning import BehavioralCloning
from expert_policy import HighwayExpert, generate_expert_trajectories
from highway.highway_env import HighwayWrapper


def load_model_and_config(model_path: str):
    """
    åŠ è½½æ¨¡å‹å’Œé…ç½®

    Args:
        model_path: æ¨¡å‹è·¯å¾„

    Returns:
        bc_model: åŠ è½½çš„æ¨¡å‹
        config: æ¨¡å‹é…ç½®
    """
    # æ‰¾åˆ°é…ç½®æ–‡ä»¶çš„è·¯å¾„
    model_dir = Path(model_path).parent
    config_path = model_dir / 'config.json'

    if not config_path.exists():
        # å¦‚æœæ²¡æœ‰é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        print("âš ï¸  Config file not found, using default configuration")
        config = {
            'env_name': 'highway-v0',
            'hidden_dims': [256, 128],
        }
    else:
        with open(config_path, 'r') as f:
            config = json.load(f)

    # åˆ›å»ºç¯å¢ƒåŒ…è£…å™¨è·å–ç»´åº¦
    env_wrapper = HighwayWrapper(config['env_name'])
    state_dim = env_wrapper.get_state_dim()
    action_dim = env_wrapper.get_action_dim()

    # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
    bc_model = BehavioralCloning(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dims=config['hidden_dims']
    )
    bc_model.load_model(model_path)

    return bc_model, config


def comprehensive_evaluation(bc_model: BehavioralCloning, config: dict,
                           num_episodes: int = 50, max_steps: int = 300,
                           render: bool = False):
    """
    å…¨é¢è¯„ä¼°è¡Œä¸ºå…‹éš†æ¨¡å‹

    Args:
        bc_model: è¡Œä¸ºå…‹éš†æ¨¡å‹
        config: é…ç½®å­—å…¸
        num_episodes: è¯„ä¼°å›åˆæ•°
        max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
        render: æ˜¯å¦æ¸²æŸ“

    Returns:
        eval_results: è¯¦ç»†çš„è¯„ä¼°ç»“æœ
    """
    print(f"ğŸ” Starting comprehensive evaluation ({num_episodes} episodes)...")

    # åŸºæœ¬æ€§èƒ½è¯„ä¼°
    basic_results = bc_model.evaluate(
        env_name=config['env_name'],
        num_episodes=num_episodes,
        max_steps=max_steps,
        render=render
    )

    # ä¸“å®¶ç­–ç•¥è¯„ä¼°ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    print("ğŸ¯ Evaluating expert policy for comparison...")
    expert_trajectories = generate_expert_trajectories(
        env_name=config['env_name'],
        num_episodes=num_episodes,
        max_steps=max_steps
    )

    expert_rewards = [traj['total_reward'] for traj in expert_trajectories]
    expert_lengths = [traj['episode_length'] for traj in expert_trajectories]

    expert_results = {
        'mean_reward': np.mean(expert_rewards),
        'std_reward': np.std(expert_rewards),
        'mean_length': np.mean(expert_lengths),
        'success_rate': len([r for r in expert_rewards if r > 0]) / len(expert_rewards),  # ç®€å•æˆåŠŸå®šä¹‰
        'episode_rewards': expert_rewards,
        'episode_lengths': expert_lengths
    }

    # åŠ¨ä½œåˆ†å¸ƒåˆ†æ
    action_distribution = analyze_action_distribution(bc_model, config, num_episodes=10)

    # é²æ£’æ€§æµ‹è¯•
    robustness_results = test_robustness(bc_model, config)

    # ç»¼åˆç»“æœ
    eval_results = {
        'basic_performance': basic_results,
        'expert_comparison': expert_results,
        'action_distribution': action_distribution,
        'robustness_test': robustness_results,
        'performance_gap': {
            'reward_gap': expert_results['mean_reward'] - basic_results['mean_reward'],
            'length_gap': expert_results['mean_length'] - basic_results['mean_length'],
            'success_rate_gap': expert_results['success_rate'] - basic_results['success_rate']
        },
        'evaluation_config': {
            'num_episodes': num_episodes,
            'max_steps': max_steps,
            'env_name': config['env_name']
        }
    }

    print("âœ… Comprehensive evaluation completed!")
    return eval_results


def analyze_action_distribution(bc_model: BehavioralCloning, config: dict, num_episodes: int = 10):
    """
    åˆ†æå­¦ä¹ ç­–ç•¥çš„åŠ¨ä½œåˆ†å¸ƒ

    Args:
        bc_model: è¡Œä¸ºå…‹éš†æ¨¡å‹
        config: é…ç½®
        num_episodes: åˆ†æçš„å›åˆæ•°

    Returns:
        action_stats: åŠ¨ä½œåˆ†å¸ƒç»Ÿè®¡
    """
    print("ğŸ“Š Analyzing action distribution...")

    env_wrapper = HighwayWrapper(config['env_name'])
    all_actions = []

    for episode in range(num_episodes):
        state, info = env_wrapper.env.reset()
        done = False
        steps = 0

        while not done and steps < 100:  # é™åˆ¶æ­¥æ•°ç”¨äºåˆ†æ
            state_flat = env_wrapper.flatten_observation(state)
            action = bc_model.policy.get_action(state_flat)
            all_actions.append(action)

            state, reward, terminated, truncated, info = env_wrapper.env.step(action)
            done = terminated or truncated
            steps += 1

    env_wrapper.env.close()

    # ç»Ÿè®¡åŠ¨ä½œåˆ†å¸ƒ
    action_counts = np.bincount(all_actions, minlength=5)  # å‡è®¾5ä¸ªåŠ¨ä½œ
    action_probs = action_counts / len(all_actions)

    action_stats = {
        'action_counts': action_counts.tolist(),
        'action_probabilities': action_probs.tolist(),
        'total_actions': len(all_actions),
        'most_frequent_action': int(np.argmax(action_counts)),
        'action_entropy': -np.sum(action_probs * np.log(action_probs + 1e-10))
    }

    return action_stats


def test_robustness(bc_model: BehavioralCloning, config: dict):
    """
    æµ‹è¯•æ¨¡å‹çš„é²æ£’æ€§ï¼ˆä¸åŒç¯å¢ƒé…ç½®ä¸‹çš„è¡¨ç°ï¼‰

    Args:
        bc_model: è¡Œä¸ºå…‹éš†æ¨¡å‹
        config: é…ç½®

    Returns:
        robustness_results: é²æ£’æ€§æµ‹è¯•ç»“æœ
    """
    print("ğŸ§ª Testing robustness across different configurations...")

    test_configs = [
        {'name': 'default', 'config': None},
        {'name': 'heavy_traffic', 'config': {'vehicles_count': 20, 'duration': 50}},
        {'name': 'light_traffic', 'config': {'vehicles_count': 5, 'duration': 50}},
        {'name': 'fast_pace', 'config': {'reward_speed_range': [25, 35], 'duration': 50}},
    ]

    robustness_results = {}

    for test_config in test_configs:
        print(f"  Testing {test_config['name']}...")

        results = bc_model.evaluate(
            env_name=config['env_name'],
            num_episodes=10,
            max_steps=100,
            render=False
        )

        robustness_results[test_config['name']] = {
            'mean_reward': results['mean_reward'],
            'success_rate': results['success_rate'],
            'mean_length': results['mean_length']
        }

    return robustness_results


def generate_evaluation_report(eval_results: dict, model_path: str, output_dir: str = "evaluation_results"):
    """
    ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š

    Args:
        eval_results: è¯„ä¼°ç»“æœ
        model_path: æ¨¡å‹è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
    """
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    # ç”Ÿæˆå›¾è¡¨
    generate_evaluation_plots(eval_results, str(output_path))

    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    report_path = output_path / 'evaluation_report.md'

    basic = eval_results['basic_performance']
    expert = eval_results['expert_comparison']
    gap = eval_results['performance_gap']
    action_dist = eval_results['action_distribution']
    robustness = eval_results['robustness_test']

    report = f"""# Behavioral Cloning Evaluation Report

## Overview
- **Model**: {model_path}
- **Environment**: {eval_results['evaluation_config']['env_name']}
- **Evaluation Episodes**: {eval_results['evaluation_config']['num_episodes']}
- **Max Steps per Episode**: {eval_results['evaluation_config']['max_steps']}
- **Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Performance Summary

### Basic Performance
- **Mean Episode Reward**: {basic['mean_reward']:.2f} Â± {basic['std_reward']:.2f}
- **Mean Episode Length**: {basic['mean_length']:.1f}
- **Success Rate**: {basic['success_rate']:.2%}

### Comparison with Expert
- **Expert Mean Reward**: {expert['mean_reward']:.2f} Â± {expert['std_reward']:.2f}
- **Expert Mean Length**: {expert['mean_length']:.1f}
- **Expert Success Rate**: {expert['success_rate']:.2%}

### Performance Gap
- **Reward Gap**: {gap['reward_gap']:.2f} ({gap['reward_gap']/expert['mean_reward']:.1%})
- **Length Gap**: {gap['length_gap']:.1f}
- **Success Rate Gap**: {gap['success_rate_gap']:.2%}

## Action Distribution Analysis
- **Total Actions Analyzed**: {action_dist['total_actions']}
- **Most Frequent Action**: {action_dist['most_frequent_action']}
- **Action Entropy**: {action_dist['action_entropy']:.3f}
- **Action Probabilities**: {action_dist['action_probabilities']}

## Robustness Test Results

| Configuration | Mean Reward | Success Rate | Mean Length |
|---------------|-------------|--------------|-------------|
"""

    for config_name, results in robustness.items():
        report += f"| {config_name} | {results['mean_reward']:.2f} | {results['success_rate']:.2%} | {results['mean_length']:.1f} |\n"

    report += """
## Analysis

### Performance Analysis
"""

    if abs(gap['reward_gap']) < expert['std_reward']:
        report += "- âœ… BC performance is comparable to expert (within 1 std dev)\n"
    else:
        report += "- âš ï¸  BC performance significantly differs from expert\n"

    if basic['success_rate'] > 0.7:
        report += "- âœ… Good success rate (>70%)\n"
    else:
        report += "- âš ï¸  Low success rate, may need more training data or model capacity\n"

    report += f"""
### Action Distribution Insights
- **Action Diversity**: Entropy = {action_dist['action_entropy']:.3f}"""

    if action_dist['action_entropy'] > 1.0:
        report += " (good diversity)\n"
    else:
        report += " (low diversity, may be overfitting)\n"

    # ä¿å­˜æŠ¥å‘Š
    with open(report_path, 'w') as f:
        f.write(report)

    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_path = output_path / 'detailed_results.json'
    with open(results_path, 'w') as f:
        json.dump(eval_results, f, indent=2)

    print(f"ğŸ“ Evaluation report saved to {report_path}")
    print(f"ğŸ“Š Detailed results saved to {results_path}")


def generate_evaluation_plots(eval_results: dict, output_dir: str):
    """
    ç”Ÿæˆè¯„ä¼°ç»“æœçš„å¯è§†åŒ–å›¾è¡¨

    Args:
        eval_results: è¯„ä¼°ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    """
    basic = eval_results['basic_performance']
    expert = eval_results['expert_comparison']
    robustness = eval_results['robustness_test']

    # å¥–åŠ±åˆ†å¸ƒå¯¹æ¯”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # å¥–åŠ±ç›´æ–¹å›¾å¯¹æ¯”
    axes[0, 0].hist(expert['episode_rewards'], alpha=0.7, label='Expert', bins=15, density=True)
    axes[0, 0].hist(basic['episode_rewards'], alpha=0.7, label='BC', bins=15, density=True)
    axes[0, 0].axvline(np.mean(expert['episode_rewards']), color='blue', linestyle='--', alpha=0.8)
    axes[0, 0].axvline(np.mean(basic['episode_rewards']), color='orange', linestyle='--', alpha=0.8)
    axes[0, 0].set_xlabel('Episode Reward')
    axes[0, 0].set_ylabel('Density')
    axes[0, 0].set_title('Reward Distribution Comparison')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # æ—¶é—´åºåˆ—å¯¹æ¯”
    episodes = range(len(basic['episode_rewards']))
    axes[0, 1].plot(episodes, basic['episode_rewards'], 'o-', alpha=0.7, label='BC Policy', markersize=4)
    axes[0, 1].axhline(np.mean(expert['episode_rewards']), color='blue', linestyle='--',
                       label='.1f')
    axes[0, 1].axhline(np.mean(basic['episode_rewards']), color='orange', linestyle='--',
                       label='.1f')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Reward')
    axes[0, 1].set_title('Episode Rewards Over Time')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # åŠ¨ä½œåˆ†å¸ƒ
    action_dist = eval_results['action_distribution']
    actions = range(len(action_dist['action_probabilities']))
    axes[1, 0].bar(actions, action_dist['action_probabilities'], alpha=0.7)
    axes[1, 0].set_xlabel('Action')
    axes[1, 0].set_ylabel('Probability')
    axes[1, 0].set_title('Learned Action Distribution')
    axes[1, 0].set_xticks(actions)
    axes[1, 0].grid(True, alpha=0.3)

    # é²æ£’æ€§æµ‹è¯•ç»“æœ
    configs = list(robustness.keys())
    rewards = [robustness[c]['mean_reward'] for c in configs]
    success_rates = [robustness[c]['success_rate'] for c in configs]

    x = range(len(configs))
    axes[1, 1].bar(x, rewards, alpha=0.7, label='Mean Reward', width=0.35)
    axes[1, 1].set_ylabel('Mean Reward', color='blue')
    axes[1, 1].tick_params(axis='y', labelcolor='blue')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels(configs, rotation=45)
    axes[1, 1].grid(True, alpha=0.3)

    ax2 = axes[1, 1].twinx()
    ax2.plot(x, success_rates, 'r-o', label='Success Rate', markersize=6)
    ax2.set_ylabel('Success Rate', color='red')
    ax2.tick_params(axis='y', labelcolor='red')
    ax2.set_ylim(0, 1)

    axes[1, 1].set_title('Robustness Across Configurations')

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'evaluation_plots.png'), dpi=300, bbox_inches='tight')
    plt.close()

    print(f"ğŸ“Š Evaluation plots saved to {output_dir}/evaluation_plots.png")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Evaluate Behavioral Cloning Model")
    parser.add_argument('--model-path', type=str, required=True,
                       help='Path to trained model')
    parser.add_argument('--output-dir', type=str, default='evaluation_results',
                       help='Output directory for results')
    parser.add_argument('--episodes', type=int, default=50,
                       help='Number of evaluation episodes')
    parser.add_argument('--max-steps', type=int, default=300,
                       help='Maximum steps per episode')
    parser.add_argument('--render', action='store_true',
                       help='Render evaluation episodes')

    args = parser.parse_args()

    try:
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“‚ Loading model from {args.model_path}")
        bc_model, config = load_model_and_config(args.model_path)

        # è¿›è¡Œè¯„ä¼°
        eval_results = comprehensive_evaluation(
            bc_model=bc_model,
            config=config,
            num_episodes=args.episodes,
            max_steps=args.max_steps,
            render=args.render
        )

        # ç”ŸæˆæŠ¥å‘Š
        generate_evaluation_report(eval_results, args.model_path, args.output_dir)

        # æ‰“å°å…³é”®ç»“æœ
        basic = eval_results['basic_performance']
        gap = eval_results['performance_gap']

        print("\nğŸ¯ Evaluation Summary:")
        print(".2f")
        print(".2f")
        print(".2f")
        print(f"ğŸ“‚ Results saved to: {args.output_dir}")

    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        raise


if __name__ == "__main__":
    main()
