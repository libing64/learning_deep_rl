#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Behavioral Cloning for Imitation Learning
è¡Œä¸ºå…‹éš†æ¨¡ä»¿å­¦ä¹ 

è¿™ä¸ªæ¨¡å—å®ç°äº†è¡Œä¸ºå…‹éš†ç®—æ³•ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ ä»ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ ç­–ç•¥ã€‚
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import sys
import os
from typing import List, Dict, Tuple, Optional

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from highway.highway_env import HighwayWrapper


class TrajectoryDataset(Dataset):
    """
    è½¨è¿¹æ•°æ®é›†ç±»

    ç”¨äºå°†ä¸“å®¶è½¨è¿¹è½¬æ¢ä¸º PyTorch æ•°æ®é›†æ ¼å¼
    """

    def __init__(self, trajectories: List[Dict], env_wrapper: HighwayWrapper):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        Args:
            trajectories: ä¸“å®¶è½¨è¿¹åˆ—è¡¨
            env_wrapper: ç¯å¢ƒå°è£…å™¨ï¼Œç”¨äºå¤„ç†è§‚å¯Ÿæ•°æ®
        """
        self.trajectories = trajectories
        self.env_wrapper = env_wrapper
        self.data = []

        # å¤„ç†è½¨è¿¹æ•°æ®
        for trajectory in trajectories:
            states = trajectory['states']
            actions = trajectory['actions']

            for state, action in zip(states, actions):
                # å±•å¹³çŠ¶æ€
                state_flat = self.env_wrapper.flatten_observation(state)
                self.data.append((state_flat, action))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        state, action = self.data[idx]
        return torch.FloatTensor(state), torch.LongTensor([action])


class BCPolicy(nn.Module):
    """
    è¡Œä¸ºå…‹éš†ç­–ç•¥ç½‘ç»œ

    ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºå°†çŠ¶æ€æ˜ å°„åˆ°åŠ¨ä½œ
    """

    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [256, 128]):
        """
        åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œ

        Args:
            state_dim: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_dim: åŠ¨ä½œç©ºé—´ç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
        """
        super(BCPolicy, self).__init__()

        # æ„å»ºç½‘ç»œå±‚
        layers = []
        input_dim = state_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(0.1)
            ])
            input_dim = hidden_dim

        # è¾“å‡ºå±‚
        layers.append(nn.Linear(input_dim, action_dim))

        self.network = nn.Sequential(*layers)

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

    def forward(self, state):
        """
        å‰å‘ä¼ æ’­

        Args:
            state: çŠ¶æ€å¼ é‡

        Returns:
            logits: åŠ¨ä½œ logits
        """
        return self.network(state)

    def get_action(self, state, deterministic=True):
        """
        è·å–åŠ¨ä½œ

        Args:
            state: çŠ¶æ€
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥

        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
        """
        self.eval()
        with torch.no_grad():
            if not isinstance(state, torch.Tensor):
                state = torch.FloatTensor(state).unsqueeze(0)

            logits = self.forward(state)
            if deterministic:
                action = torch.argmax(logits, dim=-1).item()
            else:
                probs = torch.softmax(logits, dim=-1)
                action = torch.multinomial(probs, 1).item()

        return action

    def get_action_probs(self, state):
        """
        è·å–åŠ¨ä½œæ¦‚ç‡

        Args:
            state: çŠ¶æ€

        Returns:
            probs: åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        """
        self.eval()
        with torch.no_grad():
            if not isinstance(state, torch.Tensor):
                state = torch.FloatTensor(state).unsqueeze(0)

            logits = self.forward(state)
            probs = torch.softmax(logits, dim=-1).squeeze(0)

        return probs.numpy()


class BehavioralCloning:
    """
    è¡Œä¸ºå…‹éš†ç®—æ³•å®ç°
    """

    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [256, 128],
                 learning_rate: float = 1e-3, weight_decay: float = 1e-4):
        """
        åˆå§‹åŒ–è¡Œä¸ºå…‹éš†

        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
        """
        self.state_dim = state_dim
        self.action_dim = action_dim

        # åˆ›å»ºç­–ç•¥ç½‘ç»œ
        self.policy = BCPolicy(state_dim, action_dim, hidden_dims)

        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = optim.Adam(self.policy.parameters(),
                                   lr=learning_rate,
                                   weight_decay=weight_decay)
        self.criterion = nn.CrossEntropyLoss()

        # è®­ç»ƒå†å²
        self.train_history = {
            'loss': [],
            'accuracy': []
        }

    def train(self, trajectories: List[Dict], env_wrapper: HighwayWrapper,
              batch_size: int = 64, epochs: int = 50, validation_split: float = 0.2):
        """
        è®­ç»ƒè¡Œä¸ºå…‹éš†æ¨¡å‹

        Args:
            trajectories: ä¸“å®¶è½¨è¿¹
            env_wrapper: ç¯å¢ƒå°è£…å™¨
            batch_size: æ‰¹æ¬¡å¤§å°
            epochs: è®­ç»ƒè½®æ•°
            validation_split: éªŒè¯é›†æ¯”ä¾‹
        """
        # åˆ›å»ºæ•°æ®é›†
        dataset = TrajectoryDataset(trajectories, env_wrapper)

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_size = int(len(dataset) * validation_split)
        train_size = len(dataset) - val_size

        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        print(f"ğŸ“š Dataset: {len(dataset)} samples")
        print(f"   Train: {train_size}, Validation: {val_size}")

        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self._train_epoch(train_loader)

            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self._validate_epoch(val_loader)

            # è®°å½•å†å²
            self.train_history['loss'].append(train_loss)
            self.train_history['accuracy'].append(train_acc)

            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1:3d}/{epochs} | "
                      f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.3f} | "
                      f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.3f}")

    def _train_epoch(self, data_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.policy.train()
        total_loss = 0
        total_correct = 0
        total_samples = 0

        for states, actions in data_loader:
            self.optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            logits = self.policy(states)
            loss = self.criterion(logits, actions.squeeze())

            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=-1)
            total_correct += (preds == actions.squeeze()).sum().item()
            total_samples += len(states)

        avg_loss = total_loss / len(data_loader)
        accuracy = total_correct / total_samples

        return avg_loss, accuracy

    def _validate_epoch(self, data_loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.policy.eval()
        total_loss = 0
        total_correct = 0
        total_samples = 0

        with torch.no_grad():
            for states, actions in data_loader:
                logits = self.policy(states)
                loss = self.criterion(logits, actions.squeeze())

                total_loss += loss.item()
                preds = torch.argmax(logits, dim=-1)
                total_correct += (preds == actions.squeeze()).sum().item()
                total_samples += len(states)

        avg_loss = total_loss / len(data_loader)
        accuracy = total_correct / total_samples

        return avg_loss, accuracy

    def evaluate(self, env_name: str = 'highway-v0', num_episodes: int = 10,
                max_steps: int = 200, render: bool = False):
        """
        è¯„ä¼°å­¦ä¹ åˆ°çš„ç­–ç•¥

        Args:
            env_name: ç¯å¢ƒåç§°
            num_episodes: è¯„ä¼°å›åˆæ•°
            max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
            render: æ˜¯å¦æ¸²æŸ“

        Returns:
            eval_results: è¯„ä¼°ç»“æœå­—å…¸
        """
        env = HighwayWrapper(env_name, render_mode='human' if render else None)

        episode_rewards = []
        episode_lengths = []
        success_count = 0

        print("ğŸ” Evaluating learned policy...")

        for episode in range(num_episodes):
            state, info = env.reset()
            episode_reward = 0
            steps = 0
            done = False

            while not done and steps < max_steps:
                # ä½¿ç”¨å­¦ä¹ åˆ°çš„ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                state_flat = env.flatten_observation(state)
                action = self.policy.get_action(state_flat)

                if render:
                    env.render()

                state, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated

                episode_reward += reward
                steps += 1

            episode_rewards.append(episode_reward)
            episode_lengths.append(steps)

            # å®šä¹‰æˆåŠŸæ ‡å‡†ï¼ˆæ— ç¢°æ’ä¸”è¾¾åˆ°ä¸€å®šæ­¥æ•°ï¼‰
            success = steps >= max_steps * 0.8 and not terminated  # å‡è®¾ terminated è¡¨ç¤ºç¢°æ’
            if success:
                success_count += 1

            if episode % 5 == 0:
                print(f"  Episode {episode + 1}: Reward = {episode_reward:.2f}, Steps = {steps}")

        env.close()

        eval_results = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'success_rate': success_count / num_episodes,
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths
        }

        print("âœ… Evaluation completed!")
        print(".2f")
        print(".2f")
        return eval_results

    def save_model(self, path: str):
        """
        ä¿å­˜æ¨¡å‹

        Args:
            path: ä¿å­˜è·¯å¾„
        """
        torch.save({
            'policy_state_dict': self.policy.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'state_dim': self.state_dim,
            'action_dim': self.action_dim,
            'train_history': self.train_history
        }, path)
        print(f"ğŸ’¾ Model saved to {path}")

    def load_model(self, path: str):
        """
        åŠ è½½æ¨¡å‹

        Args:
            path: æ¨¡å‹è·¯å¾„
        """
        checkpoint = torch.load(path, weights_only=False)
        self.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.train_history = checkpoint.get('train_history', {'loss': [], 'accuracy': []})
        print(f"ğŸ“‚ Model loaded from {path}")


def compare_with_expert(expert_trajectories: List[Dict], bc_results: Dict):
    """
    ä¸ä¸“å®¶ç­–ç•¥æ¯”è¾ƒæ€§èƒ½

    Args:
        expert_trajectories: ä¸“å®¶è½¨è¿¹
        bc_results: BC è¯„ä¼°ç»“æœ
    """
    expert_rewards = [traj['total_reward'] for traj in expert_trajectories]
    expert_lengths = [traj['episode_length'] for traj in expert_trajectories]

    print("ğŸ† Performance Comparison:")
    print("Expert Policy:")
    print(".2f")
    print(".1f")
    print("Behavioral Cloning:")
    print(".2f")
    print(".1f")

    # è®¡ç®—æ€§èƒ½å·®è·
    reward_gap = np.mean(expert_rewards) - bc_results['mean_reward']
    length_gap = np.mean(expert_lengths) - bc_results['mean_length']

    print("Performance Gap:")
    print(".2f")
    print(".1f")
    return reward_gap, length_gap
