#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Data Collection for Imitation Learning
æ¨¡ä»¿å­¦ä¹ æ•°æ®æ”¶é›†

è¿™ä¸ªæ¨¡å—è´Ÿè´£ç”Ÿæˆå’Œå¤„ç†ä¸“å®¶æ¼”ç¤ºæ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
- ç”Ÿæˆä¸“å®¶è½¨è¿¹
- æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
- æ•°æ®ä¿å­˜å’ŒåŠ è½½
- æ•°æ®é›†ç»Ÿè®¡åˆ†æ
"""

import numpy as np
import pickle
import json
import os
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import argparse

from expert_policy import generate_expert_trajectories, HighwayExpert
from highway.highway_env import HighwayWrapper


class ImitationDataset:
    """
    æ¨¡ä»¿å­¦ä¹ æ•°æ®é›†ç®¡ç†ç±»
    """

    def __init__(self, data_dir: str = "data"):
        """
        åˆå§‹åŒ–æ•°æ®é›†ç®¡ç†å™¨

        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        # æ•°æ®æ–‡ä»¶è·¯å¾„
        self.trajectories_file = self.data_dir / "trajectories.pkl"
        self.metadata_file = self.data_dir / "metadata.json"
        self.stats_file = self.data_dir / "statistics.json"

    def collect_expert_data(self, env_name: str = 'highway-v0',
                           num_episodes: int = 1000,
                           max_steps: int = 200,
                           save_frequency: int = 100):
        """
        æ”¶é›†ä¸“å®¶æ¼”ç¤ºæ•°æ®

        Args:
            env_name: ç¯å¢ƒåç§°
            num_episodes: æ”¶é›†çš„å›åˆæ•°
            max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
            save_frequency: ä¿å­˜é¢‘ç‡
        """
        print(f"ğŸ¯ Collecting {num_episodes} expert trajectories from {env_name}")

        all_trajectories = []
        total_samples = 0

        # åˆ†æ‰¹æ”¶é›†æ•°æ®
        batch_size = save_frequency
        for start_episode in range(0, num_episodes, batch_size):
            end_episode = min(start_episode + batch_size, num_episodes)
            current_batch = end_episode - start_episode

            print(f"\nğŸ“Š Collecting episodes {start_episode + 1}-{end_episode}")

            # ç”Ÿæˆå½“å‰æ‰¹æ¬¡çš„è½¨è¿¹
            trajectories = generate_expert_trajectories(
                env_name=env_name,
                num_episodes=current_batch,
                max_steps=max_steps
            )

            all_trajectories.extend(trajectories)
            total_samples += sum(len(traj['states']) for traj in trajectories)

            # ä¸­é—´ä¿å­˜
            if len(all_trajectories) % save_frequency == 0 or end_episode == num_episodes:
                self._save_trajectories(all_trajectories, env_name)
                print(f"ğŸ’¾ Saved {len(all_trajectories)} trajectories ({total_samples} samples)")

        # ä¿å­˜æœ€ç»ˆæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
        self._save_trajectories(all_trajectories, env_name)
        self._compute_and_save_statistics(all_trajectories, env_name)

        print("âœ… Data collection completed!")
        print(f"ğŸ“ˆ Total trajectories: {len(all_trajectories)}")
        print(f"ğŸ“ˆ Total samples: {total_samples}")

        return all_trajectories

    def _save_trajectories(self, trajectories: List[Dict], env_name: str):
        """
        ä¿å­˜è½¨è¿¹æ•°æ®

        Args:
            trajectories: è½¨è¿¹åˆ—è¡¨
            env_name: ç¯å¢ƒåç§°
        """
        # ä¿å­˜è½¨è¿¹æ•°æ®
        with open(self.trajectories_file, 'wb') as f:
            pickle.dump({
                'trajectories': trajectories,
                'env_name': env_name,
                'num_trajectories': len(trajectories),
                'total_samples': sum(len(traj['states']) for traj in trajectories)
            }, f)

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'env_name': env_name,
            'num_trajectories': len(trajectories),
            'total_samples': sum(len(traj['states']) for traj in trajectories),
            'avg_episode_length': np.mean([len(traj['states']) for traj in trajectories]),
            'avg_episode_reward': np.mean([traj['total_reward'] for traj in trajectories]),
            'max_episode_reward': max(traj['total_reward'] for traj in trajectories),
            'min_episode_reward': min(traj['total_reward'] for traj in trajectories),
        }

        with open(self.metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)

    def _compute_and_save_statistics(self, trajectories: List[Dict], env_name: str):
        """
        è®¡ç®—å¹¶ä¿å­˜æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯

        Args:
            trajectories: è½¨è¿¹åˆ—è¡¨
            env_name: ç¯å¢ƒåç§°
        """
        # åŸºæœ¬ç»Ÿè®¡
        episode_lengths = [len(traj['states']) for traj in trajectories]
        episode_rewards = [traj['total_reward'] for traj in trajectories]

        # åŠ¨ä½œåˆ†å¸ƒç»Ÿè®¡
        all_actions = []
        for traj in trajectories:
            all_actions.extend(traj['actions'])
        action_counts = np.bincount(all_actions, minlength=5)  # å‡è®¾æœ‰5ä¸ªåŠ¨ä½œ

        # çŠ¶æ€ç»Ÿè®¡
        all_states = []
        for traj in trajectories:
            for state in traj['states']:
                if isinstance(state, dict):
                    # è½¬æ¢ä¸ºæ•°ç»„
                    env_wrapper = HighwayWrapper(env_name)
                    state_flat = env_wrapper.flatten_observation(state)
                    all_states.append(state_flat)
                else:
                    all_states.append(state)

        all_states = np.array(all_states)

        statistics = {
            'basic_stats': {
                'num_trajectories': len(trajectories),
                'total_samples': len(all_states),
                'avg_episode_length': float(np.mean(episode_lengths)),
                'std_episode_length': float(np.std(episode_lengths)),
                'avg_episode_reward': float(np.mean(episode_rewards)),
                'std_episode_reward': float(np.std(episode_rewards)),
                'max_episode_reward': float(np.max(episode_rewards)),
                'min_episode_reward': float(np.min(episode_rewards)),
            },
            'action_distribution': {
                'action_counts': action_counts.tolist(),
                'action_probabilities': (action_counts / len(all_actions)).tolist(),
                'most_common_action': int(np.argmax(action_counts)),
            },
            'state_stats': {
                'state_dim': all_states.shape[1],
                'state_mean': all_states.mean(axis=0).tolist(),
                'state_std': all_states.std(axis=0).tolist(),
                'state_min': all_states.min(axis=0).tolist(),
                'state_max': all_states.max(axis=0).tolist(),
            },
            'episode_length_distribution': {
                'percentiles': {
                    '25th': float(np.percentile(episode_lengths, 25)),
                    '50th': float(np.percentile(episode_lengths, 50)),
                    '75th': float(np.percentile(episode_lengths, 75)),
                    '90th': float(np.percentile(episode_lengths, 90)),
                    '95th': float(np.percentile(episode_lengths, 95)),
                }
            }
        }

        with open(self.stats_file, 'w') as f:
            json.dump(statistics, f, indent=2)

        print("ğŸ“Š Dataset statistics computed and saved")

    def load_data(self) -> Tuple[List[Dict], Dict]:
        """
        åŠ è½½æ•°æ®é›†

        Returns:
            trajectories: è½¨è¿¹åˆ—è¡¨
            metadata: å…ƒæ•°æ®å­—å…¸
        """
        if not self.trajectories_file.exists():
            raise FileNotFoundError(f"Data file not found: {self.trajectories_file}")

        # åŠ è½½è½¨è¿¹æ•°æ®
        with open(self.trajectories_file, 'rb') as f:
            data = pickle.load(f)
            trajectories = data['trajectories']
            metadata = data

        # å¦‚æœå…ƒæ•°æ®æ–‡ä»¶å­˜åœ¨ï¼ŒåŠ è½½æ›´è¯¦ç»†çš„å…ƒæ•°æ®
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                detailed_metadata = json.load(f)
                metadata.update(detailed_metadata)

        print(f"ğŸ“‚ Loaded {len(trajectories)} trajectories with {metadata['total_samples']} samples")

        return trajectories, metadata

    def get_statistics(self) -> Dict:
        """
        è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯

        Returns:
            statistics: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.stats_file.exists():
            raise FileNotFoundError(f"Statistics file not found: {self.stats_file}")

        with open(self.stats_file, 'r') as f:
            return json.load(f)

    def validate_data(self) -> bool:
        """
        éªŒè¯æ•°æ®é›†å®Œæ•´æ€§å’Œæ­£ç¡®æ€§

        Returns:
            is_valid: æ•°æ®é›†æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            trajectories, metadata = self.load_data()
            stats = self.get_statistics()

            # åŸºæœ¬éªŒè¯
            assert len(trajectories) == metadata['num_trajectories']
            assert stats['basic_stats']['num_trajectories'] == len(trajectories)

            # è½¨è¿¹éªŒè¯
            for i, traj in enumerate(trajectories):
                assert 'states' in traj
                assert 'actions' in traj
                assert 'rewards' in traj
                assert len(traj['states']) == len(traj['actions']) == len(traj['rewards'])
                assert len(traj['states']) == traj['episode_length']

            print("âœ… Dataset validation passed")
            return True

        except Exception as e:
            print(f"âŒ Dataset validation failed: {e}")
            return False

    def split_data(self, train_ratio: float = 0.7, val_ratio: float = 0.2) \
            -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """
        å°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†

        Args:
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹

        Returns:
            train_trajectories, val_trajectories, test_trajectories: åˆ†å‰²åçš„è½¨è¿¹
        """
        trajectories, _ = self.load_data()

        # æ‰“ä¹±æ•°æ®
        np.random.shuffle(trajectories)

        n_total = len(trajectories)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)

        train_trajectories = trajectories[:n_train]
        val_trajectories = trajectories[n_train:n_train + n_val]
        test_trajectories = trajectories[n_train + n_val:]

        print(f"ğŸ“Š Data split: Train {len(train_trajectories)}, "
              f"Val {len(val_trajectories)}, Test {len(test_trajectories)}")

        return train_trajectories, val_trajectories, test_trajectories

    def sample_batch(self, batch_size: int, trajectories: Optional[List[Dict]] = None) \
            -> Tuple[np.ndarray, np.ndarray]:
        """
        ä»è½¨è¿¹ä¸­é‡‡æ ·æ‰¹æ¬¡æ•°æ®

        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            trajectories: è½¨è¿¹åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰

        Returns:
            states: çŠ¶æ€æ‰¹æ¬¡
            actions: åŠ¨ä½œæ‰¹æ¬¡
        """
        if trajectories is None:
            trajectories, _ = self.load_data()

        # å±•å¹³æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹
        all_states = []
        all_actions = []

        for traj in trajectories:
            # åˆ›å»ºç¯å¢ƒåŒ…è£…å™¨æ¥å¤„ç†çŠ¶æ€
            env_wrapper = HighwayWrapper('highway-v0')  # é»˜è®¤ç¯å¢ƒ

            for state, action in zip(traj['states'], traj['actions']):
                state_flat = env_wrapper.flatten_observation(state)
                all_states.append(state_flat)
                all_actions.append(action)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_states = np.array(all_states)
        all_actions = np.array(all_actions)

        # éšæœºé‡‡æ ·
        indices = np.random.choice(len(all_states), size=batch_size, replace=False)

        return all_states[indices], all_actions[indices]


def create_balanced_dataset(data_dir: str = "data", target_samples_per_action: int = 1000):
    """
    åˆ›å»ºåŠ¨ä½œå¹³è¡¡çš„æ•°æ®é›†

    Args:
        data_dir: æ•°æ®ç›®å½•
        target_samples_per_action: æ¯ä¸ªåŠ¨ä½œçš„ç›®æ ‡æ ·æœ¬æ•°
    """
    dataset = ImitationDataset(data_dir)
    trajectories, metadata = dataset.load_data()

    # ç»Ÿè®¡å½“å‰åŠ¨ä½œåˆ†å¸ƒ
    action_counts = np.zeros(5)  # å‡è®¾5ä¸ªåŠ¨ä½œ
    for traj in trajectories:
        counts = np.bincount(traj['actions'], minlength=5)
        action_counts += counts

    print(f"Current action distribution: {action_counts}")

    # æ‰¾å‡ºéœ€è¦é¢å¤–é‡‡æ ·çš„åŠ¨ä½œ
    min_count = np.min(action_counts)
    if min_count >= target_samples_per_action:
        print("Dataset already balanced")
        return

    # ä¸ºæ¯ä¸ªåŠ¨ä½œç”Ÿæˆé¢å¤–æ•°æ®
    additional_trajectories = []
    expert = HighwayExpert('highway-v0')

    for action in range(5):
        needed = target_samples_per_action - int(action_counts[action])
        if needed <= 0:
            continue

        print(f"Generating {needed} additional samples for action {action}")

        # ç”Ÿæˆåå‘ç‰¹å®šåŠ¨ä½œçš„è½¨è¿¹
        action_specific_trajectories = generate_action_specific_trajectories(
            expert, action, needed
        )
        additional_trajectories.extend(action_specific_trajectories)

    # åˆå¹¶å¹¶ä¿å­˜
    balanced_trajectories = trajectories + additional_trajectories
    dataset._save_trajectories(balanced_trajectories, 'highway-v0')
    dataset._compute_and_save_statistics(balanced_trajectories, 'highway-v0')

    print(f"âœ… Balanced dataset created with {len(balanced_trajectories)} trajectories")


def generate_action_specific_trajectories(expert: HighwayExpert, target_action: int,
                                        num_samples: int) -> List[Dict]:
    """
    ç”Ÿæˆåå‘ç‰¹å®šåŠ¨ä½œçš„è½¨è¿¹

    Args:
        expert: ä¸“å®¶ç­–ç•¥
        target_action: ç›®æ ‡åŠ¨ä½œ
        num_samples: éœ€è¦çš„æ ·æœ¬æ•°

    Returns:
        trajectories: ç”Ÿæˆçš„è½¨è¿¹åˆ—è¡¨
    """
    trajectories = []
    collected_samples = 0

    while collected_samples < num_samples:
        # é‡ç½®ç¯å¢ƒ
        state, info = expert.env.reset()
        episode_states = []
        episode_actions = []
        episode_rewards = []

        done = False
        steps = 0
        max_steps = 100  # é™åˆ¶episodeé•¿åº¦

        while not done and steps < max_steps:
            episode_states.append(state.copy() if hasattr(state, 'copy') else state)

            # æœ‰ä¸€å®šæ¦‚ç‡ä½¿ç”¨ç›®æ ‡åŠ¨ä½œ
            if np.random.random() < 0.7:  # 70%æ¦‚ç‡ä½¿ç”¨ç›®æ ‡åŠ¨ä½œ
                action = target_action
            else:
                action = expert.get_action(state)

            episode_actions.append(action)

            state, reward, terminated, truncated, info = expert.env.step(action)
            done = terminated or truncated
            episode_rewards.append(reward)
            steps += 1

        if len(episode_states) > 0:
            trajectories.append({
                'states': np.array(episode_states),
                'actions': np.array(episode_actions),
                'rewards': np.array(episode_rewards),
                'episode_length': len(episode_states),
                'total_reward': sum(episode_rewards)
            })
            collected_samples += len(episode_states)

    return trajectories


def main():
    """ä¸»å‡½æ•°ï¼šå‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="Imitation Learning Data Collection")
    parser.add_argument('--env', type=str, default='highway-v0',
                       help='Environment name')
    parser.add_argument('--episodes', type=int, default=500,
                       help='Number of episodes to collect')
    parser.add_argument('--max-steps', type=int, default=200,
                       help='Maximum steps per episode')
    parser.add_argument('--data-dir', type=str, default='data',
                       help='Data directory')
    parser.add_argument('--validate', action='store_true',
                       help='Validate existing dataset')
    parser.add_argument('--balance', action='store_true',
                       help='Create balanced dataset')

    args = parser.parse_args()

    dataset = ImitationDataset(args.data_dir)

    if args.validate:
        # éªŒè¯æ•°æ®é›†
        is_valid = dataset.validate_data()
        if is_valid:
            stats = dataset.get_statistics()
            print("ğŸ“Š Dataset Statistics:")
            print(f"  Trajectories: {stats['basic_stats']['num_trajectories']}")
            print(f"  Total samples: {stats['basic_stats']['total_samples']}")
            print(f"  Avg episode length: {stats['basic_stats']['avg_episode_length']:.1f}")
            print(f"  Avg episode reward: {stats['basic_stats']['avg_episode_reward']:.2f}")
            print(f"  Action distribution: {stats['action_distribution']['action_probabilities']}")

    elif args.balance:
        # åˆ›å»ºå¹³è¡¡æ•°æ®é›†
        create_balanced_dataset(args.data_dir)

    else:
        # æ”¶é›†æ•°æ®
        trajectories = dataset.collect_expert_data(
            env_name=args.env,
            num_episodes=args.episodes,
            max_steps=args.max_steps
        )

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = dataset.get_statistics()
        print("\nğŸ“Š Collection Summary:")
        print(f"  Total trajectories: {stats['basic_stats']['num_trajectories']}")
        print(f"  Total samples: {stats['basic_stats']['total_samples']}")
        print(f"  Average episode length: {stats['basic_stats']['avg_episode_length']:.1f}")
        print(f"  Average episode reward: {stats['basic_stats']['avg_episode_reward']:.2f}")


if __name__ == "__main__":
    main()
