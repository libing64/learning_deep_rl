#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Expert Policy for Imitation Learning
æ¨¡ä»¿å­¦ä¹ ä¸“å®¶ç­–ç•¥

è¿™ä¸ªæ¨¡å—å®ç°äº†åœ¨ highway ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½çš„ä¸“å®¶ç­–ç•¥ï¼Œ
ç”¨äºç”Ÿæˆæ¼”ç¤ºæ•°æ®ä¾›æ¨¡ä»¿å­¦ä¹ ç®—æ³•ä½¿ç”¨ã€‚
"""

import numpy as np
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ highway_env æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from highway.highway_env import HighwayWrapper


class HighwayExpert:
    """
    Highway ç¯å¢ƒä¸“å®¶ç­–ç•¥

    ä¸“å®¶ç­–ç•¥åŸºäºè§„åˆ™å®ç°å®‰å…¨é«˜æ•ˆçš„é©¾é©¶è¡Œä¸ºï¼š
    - ä¿æŒåœ¨å³ä¾§è½¦é“ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    - ä¿æŒå®‰å…¨è·ç¦»
    - é€‚å½“åŠ é€Ÿä»¥ä¿æŒæµé‡
    - é¿å…ç¢°æ’
    """

    def __init__(self, env_name='highway-v0', config=None):
        """
        åˆå§‹åŒ–ä¸“å®¶ç­–ç•¥

        Args:
            env_name: ç¯å¢ƒåç§°
            config: ç¯å¢ƒé…ç½®
        """
        self.env_name = env_name
        self.config = config or self._get_expert_config()
        self.env = HighwayWrapper(env_name, config=self.config)

        # ä¸“å®¶ç­–ç•¥å‚æ•°
        self.min_safe_distance = 10.0  # æœ€å°å®‰å…¨è·ç¦»
        self.preferred_speed = 25.0    # åå¥½é€Ÿåº¦
        self.speed_tolerance = 5.0     # é€Ÿåº¦å®¹å¿åº¦
        self.lane_change_threshold = 15.0  # æ¢é“é˜ˆå€¼

    def _get_expert_config(self):
        """è·å–ä¸“å®¶ç­–ç•¥çš„æ¨èé…ç½®"""
        return {
            "observation": {
                "type": "Kinematics",
                "features": ["presence", "x", "y", "vx", "vy", "cos_h", "sin_h"],
                "features_range": {
                    "x": [-100, 100],
                    "y": [-100, 100],
                    "vx": [-20, 20],
                    "vy": [-20, 20]
                },
                "absolute": False,
                "normalize": True,
                "vehicles_count": 5
            },
            "action": {
                "type": "DiscreteMetaAction"
            },
            "lanes_count": 4,
            "vehicles_count": 15,  # æ›´å¤šçš„è½¦è¾†ä½¿ç¯å¢ƒæ›´å…·æŒ‘æˆ˜æ€§
            "duration": 60,  # æ›´é•¿çš„ episode
            "initial_spacing": 2,
            "collision_reward": -5,  # æ›´é«˜çš„ç¢°æ’æƒ©ç½š
            "right_lane_reward": 0.1,
            "high_speed_reward": 0.6,
            "reward_speed_range": [20, 30],
            "normalize_reward": False
        }

    def get_action(self, observation):
        """
        åŸºäºè§‚å¯Ÿé€‰æ‹©åŠ¨ä½œ

        Args:
            observation: ç¯å¢ƒè§‚å¯Ÿ

        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ (0-4)
                0: IDLE - ä¿æŒå½“å‰çŠ¶æ€
                1: LANE_LEFT - å·¦æ¢é“
                2: LANE_RIGHT - å³æ¢é“
                3: FASTER - åŠ é€Ÿ
                4: SLOWER - å‡é€Ÿ
        """
        # è§£æè§‚å¯Ÿæ•°æ®
        vehicles = self._parse_observation(observation)
        ego_vehicle = vehicles[0]  # è‡ªå·±çš„è½¦è¾†

        # è·å–å‰æ–¹è½¦è¾†ä¿¡æ¯
        front_vehicle = self._get_front_vehicle(vehicles, ego_vehicle)

        # å†³ç­–é€»è¾‘
        action = self._decision_logic(ego_vehicle, front_vehicle, vehicles)

        return action

    def _parse_observation(self, observation):
        """
        è§£æè§‚å¯Ÿæ•°æ®

        Args:
            observation: è§‚å¯Ÿæ•°æ® (numpy array æˆ– dict)

        Returns:
            vehicles: è½¦è¾†åˆ—è¡¨ï¼Œæ¯ä¸ªè½¦è¾†åŒ…å« [presence, x, y, vx, vy, cos_h, sin_h]
        """
        if isinstance(observation, dict):
            # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
            features = []
            for key in sorted(observation.keys()):
                if key.startswith('vehicles_'):
                    features.append(observation[key])
            obs_array = np.array(features)
        else:
            # å¦‚æœæ˜¯æ•°ç»„æ ¼å¼
            obs_array = np.array(observation)

        # å¤„ç†ä¸åŒçš„è§‚å¯Ÿæ ¼å¼
        if len(obs_array.shape) == 2:
            # å¦‚æœå·²ç»æ˜¯äºŒç»´æ•°ç»„ (vehicles_count, features_per_vehicle)
            obs_reshaped = obs_array
        elif len(obs_array.shape) == 1:
            # å¦‚æœæ˜¯ä¸€ç»´æ•°ç»„ï¼Œéœ€è¦é‡å¡‘ä¸º (vehicles_count, features_per_vehicle)
            vehicles_count = obs_array.shape[0] // 7  # 7 ä¸ªç‰¹å¾ per vehicle
            if vehicles_count == 0:
                raise ValueError(
                    f"Cannot reshape observation array of size {obs_array.shape[0]} "
                    f"into shape (vehicles_count, 7). "
                    f"Observation shape: {obs_array.shape}, "
                    f"First 10 values: {obs_array[:10]}"
                )
            obs_reshaped = obs_array.reshape(vehicles_count, 7)
        else:
            raise ValueError(
                f"Unexpected observation shape: {obs_array.shape}. "
                f"Expected 1D or 2D array."
            )

        vehicles = []
        for i in range(obs_reshaped.shape[0]):
            vehicle = obs_reshaped[i]
            vehicles.append({
                'presence': vehicle[0],
                'x': vehicle[1],
                'y': vehicle[2],
                'vx': vehicle[3],
                'vy': vehicle[4],
                'cos_h': vehicle[5],
                'sin_h': vehicle[6]
            })

        return vehicles

    def _get_front_vehicle(self, vehicles, ego_vehicle):
        """
        è·å–å‰æ–¹è½¦è¾†ä¿¡æ¯

        Args:
            vehicles: æ‰€æœ‰è½¦è¾†åˆ—è¡¨
            ego_vehicle: è‡ªå·±çš„è½¦è¾†

        Returns:
            front_vehicle: å‰æ–¹è½¦è¾†ä¿¡æ¯æˆ– None
        """
        ego_lane = self._get_vehicle_lane(ego_vehicle)
        min_distance = float('inf')
        front_vehicle = None

        for vehicle in vehicles[1:]:  # è·³è¿‡è‡ªå·±çš„è½¦è¾†
            if vehicle['presence'] < 0.5:  # è½¦è¾†ä¸å­˜åœ¨
                continue

            vehicle_lane = self._get_vehicle_lane(vehicle)

            # åªè€ƒè™‘åŒè½¦é“çš„è½¦è¾†
            if vehicle_lane != ego_lane:
                continue

            # è®¡ç®—ç›¸å¯¹è·ç¦» (å‰æ–¹ä¸ºæ­£)
            distance = vehicle['x'] - ego_vehicle['x']

            if 0 < distance < min_distance:
                min_distance = distance
                front_vehicle = vehicle

        return front_vehicle

    def _get_vehicle_lane(self, vehicle):
        """
        è·å–è½¦è¾†æ‰€åœ¨è½¦é“

        Args:
            vehicle: è½¦è¾†ä¿¡æ¯

        Returns:
            lane: è½¦é“ç¼–å· (0 ä¸ºæœ€å³è½¦é“)
        """
        # åŸºäº y åæ ‡ç¡®å®šè½¦é“
        # highway-env ä¸­è½¦é“å®½åº¦é€šå¸¸ä¸º 4
        lane_width = 4.0
        lane = int(round(vehicle['y'] / lane_width))

        # ç¡®ä¿è½¦é“ç¼–å·åœ¨æœ‰æ•ˆèŒƒå›´å†…
        max_lanes = self.config.get('lanes_count', 4)
        lane = max(0, min(lane, max_lanes - 1))

        return lane

    def _decision_logic(self, ego_vehicle, front_vehicle, all_vehicles):
        """
        å†³ç­–é€»è¾‘

        Args:
            ego_vehicle: è‡ªå·±çš„è½¦è¾†
            front_vehicle: å‰æ–¹è½¦è¾†
            all_vehicles: æ‰€æœ‰è½¦è¾†

        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
        """
        ego_speed = ego_vehicle['vx']
        ego_lane = self._get_vehicle_lane(ego_vehicle)

        # 1. æ£€æŸ¥å‰æ–¹æ˜¯å¦æœ‰è½¦è¾†
        if front_vehicle is not None:
            front_distance = front_vehicle['x'] - ego_vehicle['x']
            front_speed = front_vehicle['vx']

            # å¦‚æœè·ç¦»å¤ªè¿‘ï¼Œéœ€è¦å‡é€Ÿæˆ–æ¢é“
            if front_distance < self.min_safe_distance:
                # å°è¯•æ¢åˆ°å³ä¾§è½¦é“ï¼ˆå¦‚æœä¸æ˜¯å·²ç»åœ¨æœ€å³è½¦é“ï¼‰
                if ego_lane > 0 and self._can_change_lane(ego_vehicle, all_vehicles, direction='right'):
                    return 2  # LANE_RIGHT
                else:
                    return 4  # SLOWER - å‡é€Ÿ

            # å¦‚æœå‰æ–¹è½¦è¾†è¾ƒæ…¢ï¼Œè€ƒè™‘è¶…è½¦
            elif front_speed < ego_speed - 2 and front_distance < self.lane_change_threshold:
                # å°è¯•æ¢åˆ°å·¦ä¾§è½¦é“è¶…è½¦
                if ego_lane < self.config.get('lanes_count', 4) - 1:
                    if self._can_change_lane(ego_vehicle, all_vehicles, direction='left'):
                        return 1  # LANE_LEFT

        # 2. é€Ÿåº¦æ§åˆ¶
        if ego_speed < self.preferred_speed - self.speed_tolerance:
            return 3  # FASTER - åŠ é€Ÿ
        elif ego_speed > self.preferred_speed + self.speed_tolerance:
            return 4  # SLOWER - å‡é€Ÿ

        # 3. è½¦é“é€‰æ‹©ï¼šå€¾å‘äºå³è½¦é“
        if ego_lane > 0 and self._can_change_lane(ego_vehicle, all_vehicles, direction='right'):
            # æ£€æŸ¥å³è½¦é“æ˜¯å¦æ›´ç•…é€š
            right_lane_speed = self._get_lane_average_speed(all_vehicles, ego_lane - 1)
            current_lane_speed = self._get_lane_average_speed(all_vehicles, ego_lane)

            if right_lane_speed > current_lane_speed + 2:
                return 2  # LANE_RIGHT

        # 4. é»˜è®¤ä¿æŒå½“å‰çŠ¶æ€
        return 0  # IDLE

    def _can_change_lane(self, ego_vehicle, all_vehicles, direction='left'):
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥å®‰å…¨æ¢é“

        Args:
            ego_vehicle: è‡ªå·±çš„è½¦è¾†
            all_vehicles: æ‰€æœ‰è½¦è¾†
            direction: æ¢é“æ–¹å‘ ('left' æˆ– 'right')

        Returns:
            can_change: æ˜¯å¦å¯ä»¥æ¢é“
        """
        ego_lane = self._get_vehicle_lane(ego_vehicle)
        target_lane = ego_lane + (1 if direction == 'right' else -1)

        # æ£€æŸ¥ç›®æ ‡è½¦é“æ˜¯å¦æœ‰æ•ˆ
        max_lanes = self.config.get('lanes_count', 4)
        if target_lane < 0 or target_lane >= max_lanes:
            return False

        # æ£€æŸ¥ç›®æ ‡è½¦é“æ˜¯å¦æœ‰è½¦è¾†å¤ªè¿‘
        for vehicle in all_vehicles[1:]:
            if vehicle['presence'] < 0.5:
                continue

            vehicle_lane = self._get_vehicle_lane(vehicle)
            if vehicle_lane != target_lane:
                continue

            # è®¡ç®—ç›¸å¯¹è·ç¦»
            distance = abs(vehicle['x'] - ego_vehicle['x'])
            if distance < self.min_safe_distance * 0.8:  # æ›´ä¸¥æ ¼çš„å®‰å…¨è·ç¦»
                return False

        return True

    def _get_lane_average_speed(self, vehicles, lane):
        """
        è·å–è½¦é“å¹³å‡é€Ÿåº¦

        Args:
            vehicles: æ‰€æœ‰è½¦è¾†
            lane: è½¦é“ç¼–å·

        Returns:
            avg_speed: å¹³å‡é€Ÿåº¦
        """
        lane_speeds = []
        for vehicle in vehicles[1:]:  # è·³è¿‡è‡ªå·±çš„è½¦è¾†
            if vehicle['presence'] < 0.5:
                continue

            vehicle_lane = self._get_vehicle_lane(vehicle)
            if vehicle_lane == lane:
                lane_speeds.append(vehicle['vx'])

        return np.mean(lane_speeds) if lane_speeds else self.preferred_speed


def generate_expert_trajectories(env_name='highway-v0', num_episodes=100, max_steps=200):
    """
    ç”Ÿæˆä¸“å®¶è½¨è¿¹æ•°æ®

    Args:
        env_name: ç¯å¢ƒåç§°
        num_episodes: å›åˆæ•°é‡
        max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°

    Returns:
        trajectories: è½¨è¿¹åˆ—è¡¨ï¼Œæ¯ä¸ªè½¨è¿¹åŒ…å« states, actions, rewards
    """
    expert = HighwayExpert(env_name)
    trajectories = []

    print(f"ğŸ¯ Generating {num_episodes} expert trajectories...")

    for episode in range(num_episodes):
        if episode % 10 == 0:
            print(f"  Episode {episode}/{num_episodes}")

        states = []
        actions = []
        rewards = []

        # é‡ç½®ç¯å¢ƒ
        state, info = expert.env.reset()
        done = False
        step = 0

        while not done and step < max_steps:
            # è®°å½•çŠ¶æ€
            states.append(state.copy() if hasattr(state, 'copy') else state)

            # é€‰æ‹©åŠ¨ä½œ
            action = expert.get_action(state)
            actions.append(action)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = expert.env.step(action)
            done = terminated or truncated

            rewards.append(reward)

            state = next_state
            step += 1

        trajectories.append({
            'states': np.array(states),
            'actions': np.array(actions),
            'rewards': np.array(rewards),
            'episode_length': len(states),
            'total_reward': sum(rewards)
        })

    expert.env.close()

    print("âœ… Expert trajectories generated!")
    print(".1f")
    return trajectories


def test_expert_policy(env_name='highway-v0', num_episodes=5, render=False):
    """
    æµ‹è¯•ä¸“å®¶ç­–ç•¥æ€§èƒ½

    Args:
        env_name: ç¯å¢ƒåç§°
        num_episodes: æµ‹è¯•å›åˆæ•°
        render: æ˜¯å¦æ¸²æŸ“
    """
    expert = HighwayExpert(env_name)

    print("ğŸ§ª Testing expert policy...")

    for episode in range(num_episodes):
        state, info = expert.env.reset()
        total_reward = 0
        steps = 0
        done = False

        print(f"\nEpisode {episode + 1}:")
        while not done and steps < 100:  # é™åˆ¶æµ‹è¯•æ­¥æ•°
            action = expert.get_action(state)

            if render:
                expert.env.render()

            state, reward, terminated, truncated, info = expert.env.step(action)
            done = terminated or truncated

            total_reward += reward
            steps += 1

        print(f"  Steps: {steps}, Total Reward: {total_reward:.2f}")

    expert.env.close()
    print("âœ… Expert policy test completed!")


if __name__ == "__main__":
    # æµ‹è¯•ä¸“å®¶ç­–ç•¥
    test_expert_policy(num_episodes=2, render=False)

    # ç”Ÿæˆå°‘é‡æ¼”ç¤ºæ•°æ®ç”¨äºæµ‹è¯•
    trajectories = generate_expert_trajectories(num_episodes=5, max_steps=50)

    print(f"\nğŸ“Š Generated {len(trajectories)} trajectories")
    for i, traj in enumerate(trajectories):
        print(f"  Trajectory {i}: {traj['episode_length']} steps, reward: {traj['total_reward']:.2f}")
