#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Training Script for Behavioral Cloning
è¡Œä¸ºå…‹éš†è®­ç»ƒè„šæœ¬

è¿™ä¸ªè„šæœ¬è´Ÿè´£ï¼š
- åŠ è½½ä¸“å®¶æ¼”ç¤ºæ•°æ®
- è®­ç»ƒè¡Œä¸ºå…‹éš†æ¨¡å‹
- ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
- ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå’Œå¯è§†åŒ–
"""

import argparse
import json
import os
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import torch
from datetime import datetime

from data_collection import ImitationDataset
from behavioral_cloning import BehavioralCloning
from highway.highway_env import HighwayWrapper


def setup_experiment(exp_name: str, data_dir: str = "data", models_dir: str = "models"):
    """
    è®¾ç½®å®éªŒç›®å½•å’Œé…ç½®

    Args:
        exp_name: å®éªŒåç§°
        data_dir: æ•°æ®ç›®å½•
        models_dir: æ¨¡å‹ç›®å½•

    Returns:
        config: å®éªŒé…ç½®å­—å…¸
    """
    # åˆ›å»ºæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_dir = Path(models_dir) / f"{exp_name}_{timestamp}"
    exp_dir.mkdir(parents=True, exist_ok=True)

    # å®éªŒé…ç½®
    config = {
        'experiment_name': exp_name,
        'timestamp': timestamp,
        'exp_dir': str(exp_dir),
        'data_dir': data_dir,
        'model_path': str(exp_dir / 'bc_model.pth'),
        'config_path': str(exp_dir / 'config.json'),
        'log_path': str(exp_dir / 'training.log'),
        'plots_dir': str(exp_dir / 'plots'),
        # è®­ç»ƒè¶…å‚æ•°
        'env_name': 'highway-v0',
        'hidden_dims': [256, 128],
        'learning_rate': 1e-3,
        'weight_decay': 1e-4,
        'batch_size': 64,
        'epochs': 100,
        'validation_split': 0.2,
        # æ•°æ®é…ç½®
        'use_split': True,
        'train_ratio': 0.7,
        'val_ratio': 0.2,
    }

    # ä¿å­˜é…ç½®
    with open(config['config_path'], 'w') as f:
        json.dump(config, f, indent=2)

    # åˆ›å»ºplotsç›®å½•
    Path(config['plots_dir']).mkdir(exist_ok=True)

    return config


def train_model(config: dict):
    """
    è®­ç»ƒè¡Œä¸ºå…‹éš†æ¨¡å‹

    Args:
        config: å®éªŒé…ç½®

    Returns:
        bc_model: è®­ç»ƒå¥½çš„æ¨¡å‹
        eval_results: è¯„ä¼°ç»“æœ
    """
    print("ğŸš€ Starting Behavioral Cloning Training")
    print(f"ğŸ“ Experiment: {config['experiment_name']}")
    print(f"ğŸ“‚ Model will be saved to: {config['model_path']}")

    # åŠ è½½æ•°æ®
    print("ğŸ“š Loading dataset...")
    dataset = ImitationDataset(config['data_dir'])
    trajectories, metadata = dataset.load_data()

    print(f"ğŸ“Š Dataset: {metadata['num_trajectories']} trajectories, "
          f"{metadata['total_samples']} samples")
    print(".2f")
    # åˆ›å»ºç¯å¢ƒåŒ…è£…å™¨è·å–çŠ¶æ€ç»´åº¦
    env_wrapper = HighwayWrapper(config['env_name'])
    state_dim = env_wrapper.get_state_dim()
    action_dim = env_wrapper.get_action_dim()

    print(f"ğŸ—ï¸  Model: State dim = {state_dim}, Action dim = {action_dim}")
    print(f"   Hidden layers: {config['hidden_dims']}")

    # åˆ›å»ºè¡Œä¸ºå…‹éš†æ¨¡å‹
    bc_model = BehavioralCloning(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dims=config['hidden_dims'],
        learning_rate=config['learning_rate'],
        weight_decay=config['weight_decay']
    )

    # å‡†å¤‡è®­ç»ƒæ•°æ®
    if config['use_split']:
        train_trajectories, val_trajectories, test_trajectories = dataset.split_data(
            config['train_ratio'], config['val_ratio']
        )
        train_data = train_trajectories + val_trajectories  # åˆå¹¶ç”¨äºè®­ç»ƒ
    else:
        train_data = trajectories

    # è®­ç»ƒæ¨¡å‹
    print(f"ğŸ¯ Training for {config['epochs']} epochs with batch size {config['batch_size']}...")
    bc_model.train(
        trajectories=train_data,
        env_wrapper=env_wrapper,
        batch_size=config['batch_size'],
        epochs=config['epochs'],
        validation_split=config['validation_split']
    )

    # ä¿å­˜æ¨¡å‹
    bc_model.save_model(config['model_path'])

    # è¯„ä¼°æ¨¡å‹
    print("ğŸ” Evaluating trained model...")
    eval_results = bc_model.evaluate(
        env_name=config['env_name'],
        num_episodes=20,
        max_steps=200,
        render=False
    )

    # ä¿å­˜è¯„ä¼°ç»“æœ
    eval_path = Path(config['exp_dir']) / 'evaluation_results.json'
    with open(eval_path, 'w') as f:
        json.dump(eval_results, f, indent=2)

    # ç”Ÿæˆè®­ç»ƒæ›²çº¿
    plot_training_curves(bc_model.train_history, config['plots_dir'])

    # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”
    if 'expert_trajectories' in metadata:
        expert_trajectories = metadata['expert_trajectories']
        plot_performance_comparison(expert_trajectories, eval_results, config['plots_dir'])

    print("âœ… Training completed!")
    print(".2f")
    return bc_model, eval_results


def plot_training_curves(train_history: dict, plots_dir: str):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿

    Args:
        train_history: è®­ç»ƒå†å²
        plots_dir: å›¾è¡¨ä¿å­˜ç›®å½•
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    epochs = range(1, len(train_history['loss']) + 1)

    # æŸå¤±æ›²çº¿
    ax1.plot(epochs, train_history['loss'], 'b-', label='Training Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training Loss')
    ax1.grid(True)
    ax1.legend()

    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(epochs, train_history['accuracy'], 'r-', label='Training Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Training Accuracy')
    ax2.grid(True)
    ax2.legend()

    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'training_curves.png'), dpi=300, bbox_inches='tight')
    plt.close()

    print(f"ğŸ“Š Training curves saved to {plots_dir}/training_curves.png")


def plot_performance_comparison(expert_trajectories: list, bc_results: dict, plots_dir: str):
    """
    ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾

    Args:
        expert_trajectories: ä¸“å®¶è½¨è¿¹
        bc_results: BCè¯„ä¼°ç»“æœ
        plots_dir: å›¾è¡¨ä¿å­˜ç›®å½•
    """
    expert_rewards = [traj['total_reward'] for traj in expert_trajectories]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # å¥–åŠ±åˆ†å¸ƒå¯¹æ¯”
    ax1.hist(expert_rewards, alpha=0.7, label='Expert', bins=20, density=True)
    ax1.axvline(np.mean(expert_rewards), color='blue', linestyle='--',
                label='.1f')
    ax1.axvline(bc_results['mean_reward'], color='red', linestyle='--',
                label='.1f')
    ax1.set_xlabel('Episode Reward')
    ax1.set_ylabel('Density')
    ax1.set_title('Reward Distribution Comparison')
    ax1.legend()
    ax1.grid(True)

    # å¥–åŠ±æ—¶é—´åºåˆ—
    bc_rewards = bc_results['episode_rewards']
    ax2.plot(range(len(bc_rewards)), bc_rewards, 'r-o', alpha=0.7, label='BC Policy', markersize=3)
    ax2.axhline(np.mean(expert_rewards), color='blue', linestyle='--',
                label='.1f')
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('Reward')
    ax2.set_title('BC Policy Performance')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
    plt.close()

    print(f"ğŸ“Š Performance comparison saved to {plots_dir}/performance_comparison.png")


def generate_training_report(config: dict, eval_results: dict, train_history: dict):
    """
    ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š

    Args:
        config: å®éªŒé…ç½®
        eval_results: è¯„ä¼°ç»“æœ
        train_history: è®­ç»ƒå†å²
    """
    report_path = Path(config['exp_dir']) / 'training_report.md'

    report = f"""# Behavioral Cloning Training Report

## Experiment Overview
- **Experiment Name**: {config['experiment_name']}
- **Timestamp**: {config['timestamp']}
- **Environment**: {config['env_name']}

## Model Configuration
- **Hidden Layers**: {config['hidden_dims']}
- **Learning Rate**: {config['learning_rate']}
- **Weight Decay**: {config['weight_decay']}
- **Batch Size**: {config['batch_size']}
- **Epochs**: {config['epochs']}

## Training Results
- **Final Training Loss**: {train_history['loss'][-1]:.4f}
- **Final Training Accuracy**: {train_history['accuracy'][-1]:.4f}
- **Best Training Accuracy**: {max(train_history['accuracy']):.4f}

## Evaluation Results
- **Mean Episode Reward**: {eval_results['mean_reward']:.2f} Â± {eval_results['std_reward']:.2f}
- **Mean Episode Length**: {eval_results['mean_length']:.1f}
- **Success Rate**: {eval_results['success_rate']:.2%}

## Files Generated
- Model: `{config['model_path']}`
- Config: `{config['config_path']}`
- Training Curves: `{config['plots_dir']}/training_curves.png`
- Performance Comparison: `{config['plots_dir']}/performance_comparison.png`
- Evaluation Results: `{config['exp_dir']}/evaluation_results.json`

---
*Report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

    with open(report_path, 'w') as f:
        f.write(report)

    print(f"ğŸ“ Training report saved to {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Train Behavioral Cloning Model")
    parser.add_argument('--exp-name', type=str, default='bc_highway',
                       help='Experiment name')
    parser.add_argument('--data-dir', type=str, default='data',
                       help='Data directory')
    parser.add_argument('--models-dir', type=str, default='models',
                       help='Models directory')
    parser.add_argument('--epochs', type=int, default=100,
                       help='Number of training epochs')
    parser.add_argument('--batch-size', type=int, default=64,
                       help='Training batch size')
    parser.add_argument('--lr', type=float, default=1e-3,
                       help='Learning rate')
    parser.add_argument('--hidden-dims', type=str, default='256,128',
                       help='Hidden layer dimensions (comma-separated)')
    parser.add_argument('--no-eval', action='store_true',
                       help='Skip evaluation after training')

    args = parser.parse_args()

    # è§£æéšè—å±‚ç»´åº¦
    hidden_dims = [int(x) for x in args.hidden_dims.split(',')]

    # è®¾ç½®å®éªŒ
    config = setup_experiment(
        exp_name=args.exp_name,
        data_dir=args.data_dir,
        models_dir=args.models_dir
    )

    # æ›´æ–°é…ç½®
    config.update({
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.lr,
        'hidden_dims': hidden_dims,
    })

    # é‡æ–°ä¿å­˜é…ç½®
    with open(config['config_path'], 'w') as f:
        json.dump(config, f, indent=2)

    try:
        # è®­ç»ƒæ¨¡å‹
        bc_model, eval_results = train_model(config)

        # ç”ŸæˆæŠ¥å‘Š
        generate_training_report(config, eval_results, bc_model.train_history)

        print("ğŸ‰ Training pipeline completed successfully!")
        print(f"ğŸ“‚ Results saved to: {config['exp_dir']}")

    except Exception as e:
        print(f"âŒ Training failed: {e}")
        raise


if __name__ == "__main__":
    main()
