#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DAgger Algorithm: Imitation Learning + Reinforcement Learning
DAggerç®—æ³•ï¼šæ¨¡ä»¿å­¦ä¹  + å¼ºåŒ–å­¦ä¹ 

DAgger (Dataset Aggregation) æ˜¯ä¸€ç§ç»“åˆæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ç®—æ³•ï¼š
1. ä½¿ç”¨ä¸“å®¶æ•°æ®è®­ç»ƒåˆå§‹ç­–ç•¥ï¼ˆILé˜¶æ®µï¼‰
2. ä½¿ç”¨å½“å‰ç­–ç•¥åœ¨ç¯å¢ƒä¸­è¿è¡Œï¼Œæ”¶é›†æ–°è½¨è¿¹ï¼ˆRLé˜¶æ®µï¼‰
3. è®©ä¸“å®¶å¯¹æ–°è½¨è¿¹ä¸­çš„çŠ¶æ€è¿›è¡Œæ ‡æ³¨
4. å°†æ–°æ•°æ®åŠ å…¥æ•°æ®é›†ï¼Œé‡æ–°è®­ç»ƒç­–ç•¥
5. é‡å¤æ­¥éª¤2-4ï¼Œç›´åˆ°ç­–ç•¥æ€§èƒ½æ»¡è¶³è¦æ±‚
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, ConcatDataset
from collections import deque
from typing import List, Dict, Tuple, Optional
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from highway.highway_env import HighwayWrapper
from imitation_learning.behavioral_cloning import BCPolicy, TrajectoryDataset
from imitation_learning.expert_policy import HighwayExpert


class DAggerAgent:
    """
    DAgger ç®—æ³•æ™ºèƒ½ä½“
    
    ç»“åˆæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡è¿­ä»£æ”¹è¿›ç­–ç•¥
    """
    
    def __init__(self, 
                 state_dim: int,
                 action_dim: int,
                 lr: float = 0.001,
                 hidden_dims: List[int] = [256, 256, 128],
                 device: str = 'cpu'):
        """
        åˆå§‹åŒ– DAgger æ™ºèƒ½ä½“
        
        Args:
            state_dim: çŠ¶æ€ç©ºé—´ç»´åº¦
            action_dim: åŠ¨ä½œç©ºé—´ç»´åº¦
            lr: å­¦ä¹ ç‡
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
            device: è®¡ç®—è®¾å¤‡
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = device
        
        # åˆ›å»ºç­–ç•¥ç½‘ç»œï¼ˆä½¿ç”¨BCç­–ç•¥ç½‘ç»œç»“æ„ï¼‰
        self.policy = BCPolicy(state_dim, action_dim, hidden_dims).to(device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'loss': [],
            'accuracy': [],
            'expert_agreement': []
        }
        
        # æ•°æ®é›†ï¼ˆç´¯ç§¯æ‰€æœ‰æ”¶é›†çš„æ•°æ®ï¼‰
        self.all_trajectories = []
    
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼
            
        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
        """
        self.policy.eval()
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            action_probs = self.policy(state_tensor)
            action = action_probs.argmax(dim=1).item()
        return action
    
    def train_on_dataset(self, 
                        trajectories: List[Dict],
                        env_wrapper: HighwayWrapper,
                        epochs: int = 10,
                        batch_size: int = 64,
                        validation_split: float = 0.2):
        """
        åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒç­–ç•¥
        
        Args:
            trajectories: è½¨è¿¹åˆ—è¡¨
            env_wrapper: ç¯å¢ƒå°è£…å™¨
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            validation_split: éªŒè¯é›†æ¯”ä¾‹
        """
        # åˆ›å»ºæ•°æ®é›†
        dataset = TrajectoryDataset(trajectories, env_wrapper)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        dataset_size = len(dataset)
        val_size = int(validation_split * dataset_size)
        train_size = dataset_size - val_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.policy.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for states, actions in train_loader:
                states = states.to(self.device)
                actions = actions.squeeze().to(self.device)
                
                # å‰å‘ä¼ æ’­
                action_probs = self.policy(states)
                loss = F.cross_entropy(action_probs, actions)
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
                self.optimizer.step()
                
                # ç»Ÿè®¡
                train_loss += loss.item()
                _, predicted = action_probs.max(1)
                train_total += actions.size(0)
                train_correct += predicted.eq(actions).sum().item()
            
            # éªŒè¯é˜¶æ®µ
            self.policy.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for states, actions in val_loader:
                    states = states.to(self.device)
                    actions = actions.squeeze().to(self.device)
                    
                    action_probs = self.policy(states)
                    loss = F.cross_entropy(action_probs, actions)
                    
                    val_loss += loss.item()
                    _, predicted = action_probs.max(1)
                    val_total += actions.size(0)
                    val_correct += predicted.eq(actions).sum().item()
            
            # è®°å½•å†å²
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            train_acc = 100.0 * train_correct / train_total
            val_acc = 100.0 * val_correct / val_total
            
            self.train_history['loss'].append({
                'train': avg_train_loss,
                'val': avg_val_loss,
                'epoch': epoch
            })
            self.train_history['accuracy'].append({
                'train': train_acc,
                'val': val_acc,
                'epoch': epoch
            })
            
            if (epoch + 1) % 5 == 0:
                print(f"  Epoch {epoch + 1}/{epochs} | "
                      f"Train Loss: {avg_train_loss:.4f} | "
                      f"Train Acc: {train_acc:.2f}% | "
                      f"Val Loss: {avg_val_loss:.4f} | "
                      f"Val Acc: {val_acc:.2f}%")
    
    def collect_trajectory_with_policy(self,
                                      env: HighwayWrapper,
                                      expert: HighwayExpert,
                                      max_steps: int = 200,
                                      beta: float = 0.5) -> Dict:
        """
        ä½¿ç”¨å½“å‰ç­–ç•¥æ”¶é›†è½¨è¿¹ï¼Œå¹¶ç”¨ä¸“å®¶æ ‡æ³¨
        
        Args:
            env: ç¯å¢ƒ
            expert: ä¸“å®¶ç­–ç•¥ï¼ˆç”¨äºæ ‡æ³¨ï¼‰
            max_steps: æœ€å¤§æ­¥æ•°
            beta: ä¸“å®¶åŠ¨ä½œæ··åˆæ¯”ä¾‹ï¼ˆbeta=1.0æ—¶å®Œå…¨ä½¿ç”¨ä¸“å®¶ï¼Œbeta=0.0æ—¶å®Œå…¨ä½¿ç”¨ç­–ç•¥ï¼‰
            
        Returns:
            trajectory: è½¨è¿¹å­—å…¸
        """
        state, info = env.reset()
        state = env.flatten_observation(state)
        
        trajectory = {
            'states': [],
            'actions': [],
            'expert_actions': [],
            'rewards': []
        }
        
        done = False
        step = 0
        
        while not done and step < max_steps:
            # ä½¿ç”¨å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
            policy_action = self.select_action(state, training=True)
            
            # è·å–ä¸“å®¶åŠ¨ä½œ
            expert_action = expert.get_action(state)
            
            # æ··åˆåŠ¨ä½œï¼ˆbetaæ§åˆ¶ä¸“å®¶å‚ä¸åº¦ï¼‰
            if np.random.random() < beta:
                action = expert_action
            else:
                action = policy_action
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            next_state = env.flatten_observation(next_state)
            
            # è®°å½•è½¨è¿¹ï¼ˆä½¿ç”¨ä¸“å®¶åŠ¨ä½œä½œä¸ºæ ‡ç­¾ï¼‰
            trajectory['states'].append(state.copy())
            trajectory['actions'].append(expert_action)  # ä½¿ç”¨ä¸“å®¶åŠ¨ä½œä½œä¸ºæ ‡ç­¾
            trajectory['expert_actions'].append(expert_action)
            trajectory['rewards'].append(reward)
            
            state = next_state
            step += 1
        
        return trajectory
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy_state_dict': self.policy.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_history': self.train_history,
            'state_dim': self.state_dim,
            'action_dim': self.action_dim,
        }, filepath)
        print(f"âœ… Model saved to {filepath}")
    
    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, weights_only=False)
        self.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.train_history = checkpoint.get('train_history', {'loss': [], 'accuracy': []})
        print(f"âœ… Model loaded from {filepath}")


def run_dagger_iteration(agent: DAggerAgent,
                         env: HighwayWrapper,
                         expert: HighwayExpert,
                         num_trajectories: int,
                         max_steps: int = 200,
                         beta: float = 0.5,
                         epochs: int = 10,
                         batch_size: int = 64) -> List[Dict]:
    """
    è¿è¡Œä¸€æ¬¡ DAgger è¿­ä»£
    
    Args:
        agent: DAgger æ™ºèƒ½ä½“
        env: ç¯å¢ƒ
        expert: ä¸“å®¶ç­–ç•¥
        num_trajectories: æ”¶é›†çš„è½¨è¿¹æ•°é‡
        max_steps: æ¯è½¨è¿¹æœ€å¤§æ­¥æ•°
        beta: ä¸“å®¶åŠ¨ä½œæ··åˆæ¯”ä¾‹
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        
    Returns:
        new_trajectories: æ–°æ”¶é›†çš„è½¨è¿¹åˆ—è¡¨
    """
    print(f"ğŸ“Š Collecting {num_trajectories} trajectories with current policy...")
    
    # ä½¿ç”¨å½“å‰ç­–ç•¥æ”¶é›†è½¨è¿¹
    new_trajectories = []
    for i in range(num_trajectories):
        trajectory = agent.collect_trajectory_with_policy(
            env, expert, max_steps=max_steps, beta=beta
        )
        new_trajectories.append(trajectory)
        
        if (i + 1) % 10 == 0:
            print(f"  Collected {i + 1}/{num_trajectories} trajectories")
    
    # å°†æ–°è½¨è¿¹åŠ å…¥æ€»æ•°æ®é›†
    agent.all_trajectories.extend(new_trajectories)
    
    # åœ¨ç´¯ç§¯æ•°æ®é›†ä¸Šè®­ç»ƒ
    print(f"ğŸ“ Training on {len(agent.all_trajectories)} total trajectories...")
    agent.train_on_dataset(
        agent.all_trajectories,
        env,
        epochs=epochs,
        batch_size=batch_size
    )
    
    return new_trajectories


def dagger_algorithm(env_name: str = 'highway-v0',
                     initial_expert_trajectories: int = 100,
                     dagger_iterations: int = 5,
                     trajectories_per_iteration: int = 50,
                     max_steps: int = 200,
                     beta_schedule: Optional[List[float]] = None,
                     training_epochs: int = 10,
                     batch_size: int = 64,
                     device: str = 'cpu') -> DAggerAgent:
    """
    è¿è¡Œå®Œæ•´çš„ DAgger ç®—æ³•
    
    Args:
        env_name: ç¯å¢ƒåç§°
        initial_expert_trajectories: åˆå§‹ä¸“å®¶è½¨è¿¹æ•°é‡
        dagger_iterations: DAggerè¿­ä»£æ¬¡æ•°
        trajectories_per_iteration: æ¯æ¬¡è¿­ä»£æ”¶é›†çš„è½¨è¿¹æ•°
        max_steps: æ¯è½¨è¿¹æœ€å¤§æ­¥æ•°
        beta_schedule: betaå€¼è°ƒåº¦ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨çº¿æ€§è¡°å‡ï¼‰
        training_epochs: æ¯æ¬¡è¿­ä»£çš„è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        agent: è®­ç»ƒå¥½çš„DAggeræ™ºèƒ½ä½“
    """
    print("=" * 70)
    print("ğŸš€ DAgger Algorithm: Imitation Learning + Reinforcement Learning")
    print("=" * 70)
    
    # åˆ›å»ºç¯å¢ƒ
    env = HighwayWrapper(env_name, render_mode=None)
    state_dim = env.get_state_dim()
    action_dim = env.get_action_dim()
    
    print(f"\nğŸ“‹ Environment: {env_name}")
    print(f"   State dimension: {state_dim}")
    print(f"   Action dimension: {action_dim}")
    
    # åˆ›å»ºä¸“å®¶ç­–ç•¥
    expert = HighwayExpert(env)
    
    # åˆ›å»ºDAggeræ™ºèƒ½ä½“
    agent = DAggerAgent(state_dim, action_dim, device=device)
    
    # Betaè°ƒåº¦ï¼ˆçº¿æ€§è¡°å‡ï¼šä»1.0åˆ°0.0ï¼‰
    if beta_schedule is None:
        beta_schedule = np.linspace(1.0, 0.0, dagger_iterations + 1).tolist()
    
    # é˜¶æ®µ1: åˆå§‹ä¸“å®¶æ•°æ®æ”¶é›†å’Œè®­ç»ƒ
    print(f"\n{'='*70}")
    print("ğŸ“š Phase 1: Initial Expert Data Collection")
    print(f"{'='*70}")
    
    from imitation_learning.expert_policy import generate_expert_trajectories
    
    initial_trajectories = generate_expert_trajectories(
        env_name=env_name,
        num_episodes=initial_expert_trajectories,
        max_steps=max_steps
    )
    
    agent.all_trajectories = initial_trajectories
    
    print(f"âœ… Collected {len(initial_trajectories)} initial expert trajectories")
    print(f"ğŸ“ Training initial policy...")
    agent.train_on_dataset(
        initial_trajectories,
        env,
        epochs=training_epochs,
        batch_size=batch_size
    )
    
    # é˜¶æ®µ2: DAggerè¿­ä»£
    print(f"\n{'='*70}")
    print("ğŸ”„ Phase 2: DAgger Iterations")
    print(f"{'='*70}")
    
    for iteration in range(dagger_iterations):
        print(f"\n--- DAgger Iteration {iteration + 1}/{dagger_iterations} ---")
        print(f"Beta (expert mixing ratio): {beta_schedule[iteration]:.2f}")
        
        # è¿è¡Œä¸€æ¬¡DAggerè¿­ä»£
        new_trajectories = run_dagger_iteration(
            agent=agent,
            env=env,
            expert=expert,
            num_trajectories=trajectories_per_iteration,
            max_steps=max_steps,
            beta=beta_schedule[iteration],
            epochs=training_epochs,
            batch_size=batch_size
        )
        
        print(f"âœ… Iteration {iteration + 1} completed")
        print(f"   New trajectories: {len(new_trajectories)}")
        print(f"   Total trajectories: {len(agent.all_trajectories)}")
    
    env.close()
    
    print(f"\n{'='*70}")
    print("ğŸ‰ DAgger Algorithm Completed!")
    print(f"{'='*70}")
    print(f"ğŸ“Š Final Statistics:")
    print(f"   Total trajectories: {len(agent.all_trajectories)}")
    print(f"   Total training samples: {sum(len(t['states']) for t in agent.all_trajectories)}")
    
    return agent

