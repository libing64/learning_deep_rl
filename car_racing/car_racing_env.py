#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Car Racing Environment Wrapper
Car Racing ç¯å¢ƒå°è£…
"""

import gymnasium as gym
import numpy as np


class CarRacingWrapper:
    """
    Car Racing ç¯å¢ƒå°è£…ç±»
    
    CarRacing-v2 æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒçš„å¼ºåŒ–å­¦ä¹ æ§åˆ¶é—®é¢˜ï¼š
    - è§‚å¯Ÿç©ºé—´ï¼š96x96x3 RGBå›¾åƒ
    - åŠ¨ä½œç©ºé—´ï¼šBox(3,) è¿ç»­åŠ¨ä½œç©ºé—´ [è½¬å‘, æ²¹é—¨, åˆ¹è½¦]
    - ç›®æ ‡ï¼šåœ¨èµ›é“ä¸Šå°½å¯èƒ½å¿«åœ°è¡Œé©¶ï¼Œè·å¾—é«˜åˆ†
    """
    
    def __init__(self, render_mode=None, frameskip=4):
        """
        åˆå§‹åŒ– Car Racing ç¯å¢ƒ
        
        Args:
            render_mode: æ¸²æŸ“æ¨¡å¼ ('human' ç”¨äºå¯è§†åŒ–, None ç”¨äºè®­ç»ƒ)
            frameskip: å¸§è·³è¿‡æ•°ï¼ˆç”¨äºåŠ é€Ÿè®­ç»ƒï¼‰
        """
        self.env = gym.make('CarRacing-v2', render_mode=render_mode, frameskip=frameskip)
        self.observation_space = self.env.observation_space
        self.action_space = self.env.action_space
        self.frameskip = frameskip
        
    def reset(self, seed=None):
        """
        é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€
        
        Returns:
            observation: 96x96x3 RGBå›¾åƒ
            info: é¢å¤–ä¿¡æ¯
        """
        if seed is not None:
            return self.env.reset(seed=seed)
        return self.env.reset()
    
    def step(self, action):
        """
        æ‰§è¡ŒåŠ¨ä½œ
        
        Args:
            action: åŠ¨ä½œ 
                - è¿ç»­åŠ¨ä½œ: [è½¬å‘(-1åˆ°1), æ²¹é—¨(0åˆ°1), åˆ¹è½¦(0åˆ°1)]
                - ç¦»æ•£åŠ¨ä½œ: 0-14 (æ˜ å°„åˆ°15ä¸ªç¦»æ•£åŠ¨ä½œ)
            
        Returns:
            observation: è§‚å¯Ÿ (96x96x3 RGBå›¾åƒ)
            reward: å¥–åŠ±ï¼ˆå‰è¿›è·å¾—æ­£å¥–åŠ±ï¼Œç¢°æ’æˆ–åç¦»èµ›é“è·å¾—è´Ÿå¥–åŠ±ï¼‰
            terminated: æ˜¯å¦ç»ˆæ­¢
            truncated: æ˜¯å¦æˆªæ–­ï¼ˆè¾¾åˆ°æœ€å¤§æ­¥æ•°1000ï¼‰
            info: é¢å¤–ä¿¡æ¯
        """
        return self.env.step(action)
    
    def render(self):
        """æ¸²æŸ“ç¯å¢ƒ"""
        return self.env.render()
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        self.env.close()
    
    def get_observation_shape(self):
        """è·å–è§‚å¯Ÿç©ºé—´å½¢çŠ¶"""
        return self.observation_space.shape
    
    def get_action_dim(self):
        """è·å–åŠ¨ä½œç©ºé—´ç»´åº¦ï¼ˆè¿ç»­åŠ¨ä½œç©ºé—´ï¼‰"""
        return self.action_space.shape[0]
    
    def get_action_space_type(self):
        """è·å–åŠ¨ä½œç©ºé—´ç±»å‹"""
        return type(self.action_space).__name__
    
    def discretize_action(self, discrete_action):
        """
        å°†ç¦»æ•£åŠ¨ä½œè½¬æ¢ä¸ºè¿ç»­åŠ¨ä½œ
        
        Car Racing çš„è¿ç»­åŠ¨ä½œç©ºé—´ä¸º Box(3,):
        - action[0]: è½¬å‘ (-1.0 åˆ° 1.0)
        - action[1]: æ²¹é—¨ (0.0 åˆ° 1.0)
        - action[2]: åˆ¹è½¦ (0.0 åˆ° 1.0)
        
        æˆ‘ä»¬å°†ç¦»æ•£åŒ–ä¸º15ä¸ªåŠ¨ä½œï¼š
        - 0: æ— æ“ä½œ [0, 0, 0]
        - 1-4: å·¦è½¬ [è½¬å‘, 0, 0] (è½¬å‘: -1.0, -0.5, -0.25, -0.1)
        - 5-8: å³è½¬ [è½¬å‘, 0, 0] (è½¬å‘: 0.1, 0.25, 0.5, 1.0)
        - 9-12: å‰è¿› [0, æ²¹é—¨, 0] (æ²¹é—¨: 0.25, 0.5, 0.75, 1.0)
        - 13: å·¦è½¬+å‰è¿› [-0.5, 0.5, 0]
        - 14: å³è½¬+å‰è¿› [0.5, 0.5, 0]
        
        Args:
            discrete_action: ç¦»æ•£åŠ¨ä½œ (0-14)
            
        Returns:
            continuous_action: è¿ç»­åŠ¨ä½œ [è½¬å‘, æ²¹é—¨, åˆ¹è½¦]
        """
        action_map = {
            0: [0.0, 0.0, 0.0],      # æ— æ“ä½œ
            1: [-1.0, 0.0, 0.0],      # å·¦è½¬
            2: [-0.5, 0.0, 0.0],
            3: [-0.25, 0.0, 0.0],
            4: [-0.1, 0.0, 0.0],
            5: [0.1, 0.0, 0.0],       # å³è½¬
            6: [0.25, 0.0, 0.0],
            7: [0.5, 0.0, 0.0],
            8: [1.0, 0.0, 0.0],
            9: [0.0, 0.25, 0.0],      # å‰è¿›
            10: [0.0, 0.5, 0.0],
            11: [0.0, 0.75, 0.0],
            12: [0.0, 1.0, 0.0],
            13: [-0.5, 0.5, 0.0],     # å·¦è½¬+å‰è¿›
            14: [0.5, 0.5, 0.0],      # å³è½¬+å‰è¿›
        }
        return np.array(action_map[discrete_action], dtype=np.float32)
    
    def get_discrete_action_dim(self):
        """è·å–ç¦»æ•£åŠ¨ä½œç©ºé—´ç»´åº¦"""
        return 15
    
    def __str__(self):
        return (f"Car Racing Environment\n"
                f"  Observation space: {self.observation_space}\n"
                f"  Action space: {self.action_space}\n"
                f"  Observation shape: {self.get_observation_shape()}\n"
                f"  Action dimension: {self.get_action_dim()}\n"
                f"  Discrete action dimension: {self.get_discrete_action_dim()}")


def preprocess_observation(obs):
    """
    é¢„å¤„ç†è§‚å¯Ÿï¼ˆå›¾åƒï¼‰
    
    Args:
        obs: åŸå§‹è§‚å¯Ÿ (96x96x3, uint8, 0-255)
        
    Returns:
        processed_obs: é¢„å¤„ç†åçš„è§‚å¯Ÿ (84x84x3, float32, 0-1)
    """
    # è½¬æ¢ä¸ºfloat32å¹¶å½’ä¸€åŒ–åˆ°[0, 1]
    obs = obs.astype(np.float32) / 255.0
    
    # å¯é€‰ï¼šè°ƒæ•´å¤§å°åˆ°84x84ï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰
    # è¿™é‡Œä¿æŒåŸå§‹96x96ï¼Œä½†å¯ä»¥æ”¹ä¸º84x84
    # from PIL import Image
    # obs = np.array(Image.fromarray((obs * 255).astype(np.uint8)).resize((84, 84))) / 255.0
    
    return obs


def test_environment():
    """æµ‹è¯•ç¯å¢ƒ"""
    print("ğŸ§ª Testing Car Racing Environment...")
    
    # åˆ›å»ºç¯å¢ƒ
    env = CarRacingWrapper()
    print(env)
    
    # æµ‹è¯•éšæœºç­–ç•¥
    print("\nğŸ® Testing random policy for 2 episodes:")
    for episode in range(2):
        state, info = env.reset()
        total_reward = 0
        steps = 0
        done = False
        
        print(f"  Episode {episode + 1}:")
        print(f"    Initial state shape: {state.shape}")
        print(f"    State dtype: {state.dtype}")
        print(f"    State range: [{state.min():.2f}, {state.max():.2f}]")
        
        while not done and steps < 100:  # é™åˆ¶æ­¥æ•°ç”¨äºæµ‹è¯•
            # éšæœºé€‰æ‹©ç¦»æ•£åŠ¨ä½œ
            discrete_action = np.random.randint(0, env.get_discrete_action_dim())
            continuous_action = env.discretize_action(discrete_action)
            
            state, reward, terminated, truncated, info = env.step(continuous_action)
            total_reward += reward
            steps += 1
            done = terminated or truncated
            
        print(f"    Steps = {steps}, Total Reward = {total_reward:.2f}")
    
    env.close()
    print("\nâœ… Environment test completed!")


if __name__ == "__main__":
    test_environment()

